{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "094868a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55ed8cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# genes-pathways annotation\n",
    "\n",
    "path = './KEGG_pathways/20230205_kegg_hsa.gmt'\n",
    "\n",
    "files = open(path,encoding='utf-8')\n",
    "\n",
    "files = files.readlines()\n",
    "\n",
    "paways_genes_dict = {}\n",
    "for i in files: \n",
    "    paways_genes_dict[i.split('\\t')[0].split('_')[0]] = i.replace('\\n','').split('\\t')[2:] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "376544fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mirna-pathways annotation\n",
    "path = './KEGG_pathways/kegg_anano.txt'\n",
    "\n",
    "files = open(path,encoding='utf-8')\n",
    "\n",
    "files = files.readlines()\n",
    "\n",
    "paways_mirna_dict = {}\n",
    "for i in files:\n",
    "     keys = i.split(',')[0].split('|')[1]\n",
    "     values1 = i.split(',')[1:-1]\n",
    "     values2 =  i.split(',')[-1].replace('\\n','')\n",
    "     values1.append(values2)\n",
    "     values1 =list(set(values1)) \n",
    "     paways_mirna_dict[keys] = values1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27293b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "union_kegg = list(set(paways_genes_dict.keys()).intersection(set(paways_mirna_dict.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cde7b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "paways_genes_dicts ={}\n",
    "paways_mirna_dicts ={}\n",
    "\n",
    "for i in union_kegg:\n",
    "    paways_genes_dicts[i] = paways_genes_dict[i]\n",
    "    \n",
    "for i in union_kegg:\n",
    "    paways_mirna_dicts[i] = paways_mirna_dict[i]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30fe7bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "genes_existed_pathway = []\n",
    "\n",
    "mirna_existed_pathway = []\n",
    "\n",
    "for index in paways_genes_dicts.keys():\n",
    "    genes_existed_pathway = genes_existed_pathway+ list(paways_genes_dicts[index])\n",
    "genes_existed_pathway = set(genes_existed_pathway)\n",
    "\n",
    "\n",
    "for index in paways_mirna_dicts.keys():\n",
    "    mirna_existed_pathway = mirna_existed_pathway+ list(paways_mirna_dicts[index])\n",
    "mirna_existed_pathway = set(mirna_existed_pathway)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d8c874f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7768\n",
      "812\n"
     ]
    }
   ],
   "source": [
    "print(len(genes_existed_pathway))\n",
    "print(len(mirna_existed_pathway))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1948c50f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3877bbba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Loading data\n",
    "\n",
    "miRNA_data = pd.read_csv(\"./AML_data/miRNA_data.csv\",index_col = 0)\n",
    "\n",
    "mRNA_data = pd.read_csv(\"./AML_data/mRNA_data.csv\",index_col = 0)\n",
    "\n",
    "example_case = pd.read_csv('./AML_data/response.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b8a004d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(221, 2000)\n",
      "(221, 100)\n"
     ]
    }
   ],
   "source": [
    "print(mRNA_data.shape)\n",
    "print(miRNA_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "165fe600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mRNA_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1967e5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5295fa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "union_gene_miRNA = list(miRNA_data.columns)\n",
    "union_gene_mRNA = list(mRNA_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0b65eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathway_union = list(paways_genes_dicts.keys())\n",
    "len(pathway_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0608bea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_list = [union_gene_mRNA]\n",
    "\n",
    "gene_pathway_bp_dfs = []\n",
    "\n",
    "\n",
    "for i in range(len(mask_list)):\n",
    "    pathways_genes = np.zeros((len(pathway_union), len(mask_list[i]))) \n",
    "    for p  in pathway_union:\n",
    "        gs = paways_genes_dicts[p]\n",
    "        g_inds = [mask_list[i].index(g) for g in gs if g in mask_list[i]]\n",
    "        p_ind = pathway_union.index(p)\n",
    "        pathways_genes[p_ind, g_inds] = 1\n",
    "    gene_pathway_bp = pd.DataFrame(pathways_genes, index=pathway_union, columns=mask_list[i])\n",
    "    \n",
    "    gene_pathway_bp_dfs.append(gene_pathway_bp)\n",
    "    \n",
    "\n",
    "pathways_genes = np.zeros((len(pathway_union), len(union_gene_miRNA))) \n",
    "for p  in pathway_union:\n",
    "    gs = paways_mirna_dicts[p]\n",
    "    g_inds = [union_gene_miRNA.index(g) for g in gs if g in union_gene_miRNA]\n",
    "    p_ind = pathway_union.index(p)\n",
    "    pathways_genes[p_ind, g_inds] = 1\n",
    "gene_pathway_bp = pd.DataFrame(pathways_genes, index=pathway_union, columns=union_gene_miRNA)\n",
    "\n",
    "gene_pathway_bp_dfs.append(gene_pathway_bp)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54a9d194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(gene_pathway_bp_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15821250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gene_pathway_bp_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "125e5848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.initializers import glorot_uniform, Initializer\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Embedding, GlobalAveragePooling1D,Layer\n",
    "from tensorflow.keras import initializers,activations,regularizers\n",
    "from tensorflow.keras.regularizers import Regularizer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    " \n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11f21ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Biological_module(Layer):\n",
    "    def __init__(self, units, mapp=None, nonzero_ind=None, kernel_initializer='glorot_uniform', W_regularizer=None,\n",
    "                 activation='tanh', use_bias=True,bias_initializer='zeros', bias_regularizer=None,\n",
    "                 bias_constraint=None,**kwargs):\n",
    "        \n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.mapp = mapp\n",
    "        self.nonzero_ind = nonzero_ind\n",
    "        self.use_bias = use_bias\n",
    "        \n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.kernel_regularizer = regularizers.get(W_regularizer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activation_fn = activations.get(activation)\n",
    "        super(Biological_module, self).__init__(**kwargs)\n",
    "\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        input_dim = input_shape[1]\n",
    "   \n",
    "\n",
    "        if not self.mapp is None:\n",
    "            self.mapp = self.mapp.astype(np.float32)\n",
    "\n",
    "   \n",
    "        if self.nonzero_ind is None:\n",
    "            nonzero_ind = np.array(np.nonzero(self.mapp)).T\n",
    "            self.nonzero_ind = nonzero_ind\n",
    "\n",
    "        self.kernel_shape = (input_dim, self.units)\n",
    "        \n",
    "\n",
    "        nonzero_count = self.nonzero_ind.shape[0]   \n",
    "\n",
    "\n",
    "        self.kernel_vector = self.add_weight(name='kernel_vector',\n",
    "                                             shape=(nonzero_count,),\n",
    "                                             initializer=self.kernel_initializer,\n",
    "                                             regularizer=self.kernel_regularizer,\n",
    "                                             trainable=True)\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(shape=(self.units,),\n",
    "                                        initializer=self.bias_initializer,\n",
    "                                        name='bias',\n",
    "                                        regularizer=self.bias_regularizer\n",
    "                                        )\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "        super(Biological_module, self).build(input_shape)  \n",
    "      \n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        \n",
    "        trans = tf.scatter_nd(tf.constant(self.nonzero_ind, tf.int32), self.kernel_vector,\n",
    "                           tf.constant(list(self.kernel_shape)))\n",
    "    \n",
    "        output = K.dot(inputs, trans)\n",
    "        \n",
    "    \n",
    "        if self.use_bias:\n",
    "            output = K.bias_add(output, self.bias)\n",
    "            \n",
    "        if self.activation_fn is not None:\n",
    "            output = self.activation_fn(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'units': self.units,\n",
    "            'activation': self.activation,\n",
    "            'use_bias': self.use_bias,\n",
    "            'nonzero_ind': np.array(self.nonzero_ind),\n",
    "          \n",
    "            'bias_initializer': initializers.serialize(self.bias_initializer),\n",
    "            'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
    "\n",
    "            'kernel_initializer': initializers.serialize(self.kernel_initializer),\n",
    "            'W_regularizer': regularizers.serialize(self.kernel_regularizer),\n",
    "\n",
    "        }\n",
    "        base_config = super(Biological_module, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "      \n",
    "        return (input_shape[0], self.units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c438cb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Self_Attention(Layer):\n",
    " \n",
    "    def __init__(self, output_dim,  W_regularizer=None,**kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        self.kernel_regularizer = regularizers.get(W_regularizer)\n",
    "        super(Self_Attention, self).__init__(**kwargs)\n",
    " \n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                      shape=(3,input_shape[1], self.output_dim),\n",
    "                                      initializer='uniform',\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      trainable=True)\n",
    " \n",
    "        super(Self_Attention, self).build(input_shape)  \n",
    " \n",
    "    def call(self, x):\n",
    "        WQ = K.dot(x, self.kernel[0])\n",
    "        WK = K.dot(x, self.kernel[1])\n",
    "        WV = K.dot(x, self.kernel[2])\n",
    " \n",
    "\n",
    "        print(\"K.permute_dimensions(WK.shape\",(K.permute_dimensions(WK,[1,0]).shape))\n",
    " \n",
    "        QK =  K.dot(K.permute_dimensions(WK,[1,0]),WQ)\n",
    "    \n",
    " \n",
    "        QK = QK / (64**0.5)\n",
    " \n",
    "        QK = K.softmax(QK)\n",
    " \n",
    "        print(\"QK.shape\",QK.shape)\n",
    " \n",
    "        V = K.dot(WV,QK)\n",
    "        \n",
    "        print(V.shape)\n",
    " \n",
    "        return V\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "          \n",
    "            'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
    "\n",
    "\n",
    "        }\n",
    "        base_config = super(Self_Attention, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    " \n",
    "    def compute_output_shape(self, input_shape):\n",
    " \n",
    "        return (input_shape[0],input_shape[1],self.output_dim)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab00da6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "020e30a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(mRNA_data,miRNA_data):\n",
    "    \n",
    "\n",
    "    \n",
    "    S_inputs_mRNA = Input(shape=(mRNA_data.shape[1],), dtype='float32',name= 'mRNA_inputs')\n",
    "  \n",
    "    S_inputs_miRNA = Input(shape=(miRNA_data.shape[1],), dtype='float32',name= 'miRNA_inputs')\n",
    "    \n",
    "\n",
    "   \n",
    "    h0_mRNA = Biological_module(gene_pathway_bp_dfs[0].shape[0],mapp =gene_pathway_bp_dfs[0].values.T, name = 'h0_mRNA',W_regularizer=l2(0.001))(S_inputs_mRNA)\n",
    "\n",
    "    \n",
    "    h0_miRNA = Biological_module(gene_pathway_bp_dfs[1].shape[0],mapp =gene_pathway_bp_dfs[1].values.T, name = 'h0_miRNA',W_regularizer=l2(0.001))(S_inputs_miRNA)\n",
    "\n",
    "\n",
    "\n",
    "    atten1 = Self_Attention(64,W_regularizer=l2(0.003))(h0_mRNA)\n",
    "    atten2 = Self_Attention(64,W_regularizer=l2(0.003))(h0_miRNA)\n",
    "    \n",
    "    feature_tal = tf.keras.layers.concatenate([atten1,atten2])\n",
    "\n",
    "    \n",
    "    h4 = tf.keras.layers.Dense(32,activation='tanh')(feature_tal)\n",
    "    \n",
    "    h5 = tf.keras.layers.Dense(1,activation='sigmoid')(h4)\n",
    "    \n",
    "\n",
    "    model = Model(inputs=[S_inputs_mRNA,S_inputs_miRNA], outputs=h5)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate = 0.0001,decay=0.0001) \n",
    "    model.compile(optimizer=opt,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe8181c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1044dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation function\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import average_precision_score\n",
    "   \n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "def get_metrics(true_score,pre_score,pre_probe):\n",
    "    \n",
    "  \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(true_score, pre_probe, pos_label=1)\n",
    "   \n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    aupr = average_precision_score(true_score, pre_probe)\n",
    "    \n",
    "    pre, rec, thresholds = precision_recall_curve(true_score, pre_probe)    \n",
    "    auprc  = metrics.auc(rec, pre)\n",
    "    \n",
    "    \n",
    "    accuracy = accuracy_score(true_score,pre_score)\n",
    "    \n",
    "    f1 = metrics.f1_score(true_score, pre_score)\n",
    "    \n",
    "    precision = metrics.precision_score(true_score,pre_score)\n",
    "    \n",
    "    recall = metrics.recall_score(true_score,pre_score)\n",
    "    \n",
    "     \n",
    "    print( print(confusion_matrix(true_score,pre_score)))\n",
    "    return precision,accuracy,recall,f1,auc,aupr,auprc\n",
    "\n",
    "\n",
    "def evaluates(y_test, y_pred):\n",
    "    \n",
    "    auc = metrics.roc_auc_score(y_test,y_pred)\n",
    "    \n",
    "    aupr = average_precision_score(y_test, y_pred)\n",
    "    \n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred)    \n",
    "    auprc  = metrics.auc(recall, precision)\n",
    "    \n",
    "    pp = [1 if index>=0.5  else 0 for index in  y_pred ]\n",
    "    \n",
    "    pre = metrics.precision_score(y_test,pp)\n",
    "    \n",
    "    f1 = metrics.f1_score(y_test,pp)\n",
    "    \n",
    "    rec = metrics.recall_score(y_test,pp)\n",
    "    \n",
    "    acc = metrics.accuracy_score(y_test,pp)\n",
    "    \n",
    "    print(confusion_matrix(y_test,pp))\n",
    "    return pre,acc,rec,f1,auc,aupr,auprc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1cad9be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221 120 101\n",
      "1.0940594059405941 0.9208333333333333\n"
     ]
    }
   ],
   "source": [
    "y = example_case['response'].values\n",
    "n_samples =example_case['response'].values\n",
    "\n",
    "print(len(n_samples),n_samples.sum(),(len(n_samples) -n_samples.sum()))\n",
    "\n",
    "x_0 =  len(n_samples) / (2*  (len(n_samples) -n_samples.sum()))\n",
    "x_1 =  len(n_samples) / (2*  n_samples.sum())\n",
    "\n",
    "print(x_0,x_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80621e63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K.permute_dimensions(WK.shape (64, None)\n",
      "QK.shape (64, 64)\n",
      "(None, 64)\n",
      "K.permute_dimensions(WK.shape (64, None)\n",
      "QK.shape (64, 64)\n",
      "(None, 64)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " mRNA_inputs (InputLayer)       [(None, 2000)]       0           []                               \n",
      "                                                                                                  \n",
      " miRNA_inputs (InputLayer)      [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " h0_mRNA (Biological_module)    (None, 238)          6087        ['mRNA_inputs[0][0]']            \n",
      "                                                                                                  \n",
      " h0_miRNA (Biological_module)   (None, 238)          10267       ['miRNA_inputs[0][0]']           \n",
      "                                                                                                  \n",
      " self__attention (Self_Attentio  (None, 64)          45696       ['h0_mRNA[0][0]']                \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " self__attention_1 (Self_Attent  (None, 64)          45696       ['h0_miRNA[0][0]']               \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 128)          0           ['self__attention[0][0]',        \n",
      "                                                                  'self__attention_1[0][0]']      \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 32)           4128        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            33          ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 111,907\n",
      "Trainable params: 111,907\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 176 samples\n",
      "Epoch 1/130\n",
      "176/176 [==============================] - 0s 1ms/sample - loss: 0.9231 - acc: 0.5511\n",
      "Epoch 2/130\n",
      "176/176 [==============================] - 0s 317us/sample - loss: 0.9190 - acc: 0.5455\n",
      "Epoch 3/130\n",
      "176/176 [==============================] - 0s 402us/sample - loss: 0.9149 - acc: 0.5455\n",
      "Epoch 4/130\n",
      "176/176 [==============================] - 0s 210us/sample - loss: 0.9108 - acc: 0.5455\n",
      "Epoch 5/130\n",
      "176/176 [==============================] - 0s 317us/sample - loss: 0.9067 - acc: 0.5511\n",
      "Epoch 6/130\n",
      "176/176 [==============================] - 0s 244us/sample - loss: 0.9027 - acc: 0.6193\n",
      "Epoch 7/130\n",
      "176/176 [==============================] - 0s 238us/sample - loss: 0.8987 - acc: 0.6761\n",
      "Epoch 8/130\n",
      "176/176 [==============================] - 0s 278us/sample - loss: 0.8948 - acc: 0.6875\n",
      "Epoch 9/130\n",
      "176/176 [==============================] - 0s 283us/sample - loss: 0.8908 - acc: 0.7159\n",
      "Epoch 10/130\n",
      "176/176 [==============================] - 0s 272us/sample - loss: 0.8869 - acc: 0.7102\n",
      "Epoch 11/130\n",
      "176/176 [==============================] - 0s 289us/sample - loss: 0.8830 - acc: 0.7159\n",
      "Epoch 12/130\n",
      "176/176 [==============================] - 0s 289us/sample - loss: 0.8791 - acc: 0.7159\n",
      "Epoch 13/130\n",
      "176/176 [==============================] - 0s 340us/sample - loss: 0.8752 - acc: 0.7102\n",
      "Epoch 14/130\n",
      "176/176 [==============================] - 0s 295us/sample - loss: 0.8713 - acc: 0.7102\n",
      "Epoch 15/130\n",
      "176/176 [==============================] - 0s 300us/sample - loss: 0.8673 - acc: 0.7102\n",
      "Epoch 16/130\n",
      "176/176 [==============================] - 0s 323us/sample - loss: 0.8634 - acc: 0.7216\n",
      "Epoch 17/130\n",
      "176/176 [==============================] - 0s 249us/sample - loss: 0.8594 - acc: 0.7102\n",
      "Epoch 18/130\n",
      "176/176 [==============================] - 0s 295us/sample - loss: 0.8553 - acc: 0.7045\n",
      "Epoch 19/130\n",
      "176/176 [==============================] - 0s 272us/sample - loss: 0.8512 - acc: 0.7045\n",
      "Epoch 20/130\n",
      "176/176 [==============================] - 0s 244us/sample - loss: 0.8467 - acc: 0.7045\n",
      "Epoch 21/130\n",
      "176/176 [==============================] - 0s 312us/sample - loss: 0.8424 - acc: 0.7045\n",
      "Epoch 22/130\n",
      "176/176 [==============================] - 0s 357us/sample - loss: 0.8376 - acc: 0.7159\n",
      "Epoch 23/130\n",
      "176/176 [==============================] - 0s 425us/sample - loss: 0.8327 - acc: 0.7159\n",
      "Epoch 24/130\n",
      "176/176 [==============================] - 0s 329us/sample - loss: 0.8276 - acc: 0.7159\n",
      "Epoch 25/130\n",
      "176/176 [==============================] - 0s 323us/sample - loss: 0.8219 - acc: 0.7216\n",
      "Epoch 26/130\n",
      "176/176 [==============================] - 0s 300us/sample - loss: 0.8163 - acc: 0.7216\n",
      "Epoch 27/130\n",
      "176/176 [==============================] - 0s 351us/sample - loss: 0.8098 - acc: 0.7159\n",
      "Epoch 28/130\n",
      "176/176 [==============================] - 0s 278us/sample - loss: 0.8032 - acc: 0.7216\n",
      "Epoch 29/130\n",
      "176/176 [==============================] - 0s 550us/sample - loss: 0.7958 - acc: 0.7216\n",
      "Epoch 30/130\n",
      "176/176 [==============================] - 0s 266us/sample - loss: 0.7884 - acc: 0.7216\n",
      "Epoch 31/130\n",
      "176/176 [==============================] - 0s 272us/sample - loss: 0.7796 - acc: 0.7273\n",
      "Epoch 32/130\n",
      "176/176 [==============================] - 0s 368us/sample - loss: 0.7715 - acc: 0.7330\n",
      "Epoch 33/130\n",
      "176/176 [==============================] - 0s 187us/sample - loss: 0.7619 - acc: 0.7330\n",
      "Epoch 34/130\n",
      "176/176 [==============================] - 0s 215us/sample - loss: 0.7522 - acc: 0.7330\n",
      "Epoch 35/130\n",
      "176/176 [==============================] - 0s 215us/sample - loss: 0.7426 - acc: 0.7273\n",
      "Epoch 36/130\n",
      "176/176 [==============================] - 0s 232us/sample - loss: 0.7322 - acc: 0.7273\n",
      "Epoch 37/130\n",
      "176/176 [==============================] - 0s 181us/sample - loss: 0.7229 - acc: 0.7273\n",
      "Epoch 38/130\n",
      "176/176 [==============================] - 0s 210us/sample - loss: 0.7119 - acc: 0.7273\n",
      "Epoch 39/130\n",
      "176/176 [==============================] - 0s 193us/sample - loss: 0.7018 - acc: 0.7273\n",
      "Epoch 40/130\n",
      "176/176 [==============================] - 0s 198us/sample - loss: 0.6916 - acc: 0.7273\n",
      "Epoch 41/130\n",
      "176/176 [==============================] - 0s 215us/sample - loss: 0.6829 - acc: 0.7273\n",
      "Epoch 42/130\n",
      "176/176 [==============================] - 0s 221us/sample - loss: 0.6742 - acc: 0.7330\n",
      "Epoch 43/130\n",
      "176/176 [==============================] - 0s 187us/sample - loss: 0.6656 - acc: 0.7500\n",
      "Epoch 44/130\n",
      "176/176 [==============================] - 0s 227us/sample - loss: 0.6581 - acc: 0.7443\n",
      "Epoch 45/130\n",
      "176/176 [==============================] - 0s 238us/sample - loss: 0.6499 - acc: 0.7273\n",
      "Epoch 46/130\n",
      "176/176 [==============================] - 0s 374us/sample - loss: 0.6437 - acc: 0.7273\n",
      "Epoch 47/130\n",
      "176/176 [==============================] - 0s 300us/sample - loss: 0.6366 - acc: 0.7386\n",
      "Epoch 48/130\n",
      "176/176 [==============================] - 0s 278us/sample - loss: 0.6306 - acc: 0.7273\n",
      "Epoch 49/130\n",
      "176/176 [==============================] - 0s 306us/sample - loss: 0.6243 - acc: 0.7443\n",
      "Epoch 50/130\n",
      "176/176 [==============================] - 0s 289us/sample - loss: 0.6194 - acc: 0.7443\n",
      "Epoch 51/130\n",
      "176/176 [==============================] - 0s 289us/sample - loss: 0.6138 - acc: 0.7500\n",
      "Epoch 52/130\n",
      "176/176 [==============================] - 0s 283us/sample - loss: 0.6085 - acc: 0.7500\n",
      "Epoch 53/130\n",
      "176/176 [==============================] - 0s 283us/sample - loss: 0.6036 - acc: 0.7557\n",
      "Epoch 54/130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176/176 [==============================] - 0s 527us/sample - loss: 0.5983 - acc: 0.7614\n",
      "Epoch 55/130\n",
      "176/176 [==============================] - 0s 272us/sample - loss: 0.5944 - acc: 0.7557\n",
      "Epoch 56/130\n",
      "176/176 [==============================] - 0s 476us/sample - loss: 0.5888 - acc: 0.7614\n",
      "Epoch 57/130\n",
      "176/176 [==============================] - 0s 448us/sample - loss: 0.5843 - acc: 0.7614\n",
      "Epoch 58/130\n",
      "176/176 [==============================] - 0s 368us/sample - loss: 0.5797 - acc: 0.7614\n",
      "Epoch 59/130\n",
      "176/176 [==============================] - 0s 272us/sample - loss: 0.5750 - acc: 0.7614\n",
      "Epoch 60/130\n",
      "176/176 [==============================] - 0s 266us/sample - loss: 0.5706 - acc: 0.7727\n",
      "Epoch 61/130\n",
      "176/176 [==============================] - 0s 255us/sample - loss: 0.5661 - acc: 0.7727\n",
      "Epoch 62/130\n",
      "176/176 [==============================] - 0s 261us/sample - loss: 0.5614 - acc: 0.7670\n",
      "Epoch 63/130\n",
      "176/176 [==============================] - 0s 261us/sample - loss: 0.5573 - acc: 0.7784\n",
      "Epoch 64/130\n",
      "176/176 [==============================] - 0s 266us/sample - loss: 0.5529 - acc: 0.7784\n",
      "Epoch 65/130\n",
      "176/176 [==============================] - 0s 210us/sample - loss: 0.5486 - acc: 0.7784\n",
      "Epoch 66/130\n",
      "176/176 [==============================] - 0s 193us/sample - loss: 0.5441 - acc: 0.7784\n",
      "Epoch 67/130\n",
      "176/176 [==============================] - 0s 278us/sample - loss: 0.5398 - acc: 0.7784\n",
      "Epoch 68/130\n",
      "176/176 [==============================] - 0s 198us/sample - loss: 0.5363 - acc: 0.7670\n",
      "Epoch 69/130\n",
      "176/176 [==============================] - 0s 255us/sample - loss: 0.5316 - acc: 0.7670\n",
      "Epoch 70/130\n",
      "176/176 [==============================] - 0s 261us/sample - loss: 0.5271 - acc: 0.7727\n",
      "Epoch 71/130\n",
      "176/176 [==============================] - 0s 238us/sample - loss: 0.5225 - acc: 0.7784\n",
      "Epoch 72/130\n",
      "176/176 [==============================] - 0s 255us/sample - loss: 0.5190 - acc: 0.7898\n",
      "Epoch 73/130\n",
      "176/176 [==============================] - 0s 232us/sample - loss: 0.5159 - acc: 0.7898\n",
      "Epoch 74/130\n",
      "176/176 [==============================] - 0s 227us/sample - loss: 0.5114 - acc: 0.7898\n",
      "Epoch 75/130\n",
      "176/176 [==============================] - 0s 204us/sample - loss: 0.5077 - acc: 0.7898\n",
      "Epoch 76/130\n",
      "176/176 [==============================] - 0s 261us/sample - loss: 0.5032 - acc: 0.8011\n",
      "Epoch 77/130\n",
      "176/176 [==============================] - 0s 215us/sample - loss: 0.4992 - acc: 0.8011\n",
      "Epoch 78/130\n",
      "176/176 [==============================] - 0s 238us/sample - loss: 0.4954 - acc: 0.8011\n",
      "Epoch 79/130\n",
      "176/176 [==============================] - 0s 244us/sample - loss: 0.4918 - acc: 0.8011\n",
      "Epoch 80/130\n",
      "176/176 [==============================] - 0s 278us/sample - loss: 0.4880 - acc: 0.8068\n",
      "Epoch 81/130\n",
      "176/176 [==============================] - 0s 261us/sample - loss: 0.4850 - acc: 0.8068\n",
      "Epoch 82/130\n",
      "176/176 [==============================] - 0s 215us/sample - loss: 0.4811 - acc: 0.8068\n",
      "Epoch 83/130\n",
      "176/176 [==============================] - 0s 238us/sample - loss: 0.4773 - acc: 0.8068\n",
      "Epoch 84/130\n",
      "176/176 [==============================] - 0s 227us/sample - loss: 0.4733 - acc: 0.8068\n",
      "Epoch 85/130\n",
      "176/176 [==============================] - 0s 261us/sample - loss: 0.4707 - acc: 0.8125\n",
      "Epoch 86/130\n",
      "176/176 [==============================] - 0s 181us/sample - loss: 0.4668 - acc: 0.8125\n",
      "Epoch 87/130\n",
      "176/176 [==============================] - 0s 232us/sample - loss: 0.4643 - acc: 0.8125\n",
      "Epoch 88/130\n",
      "176/176 [==============================] - 0s 204us/sample - loss: 0.4601 - acc: 0.8068\n",
      "Epoch 89/130\n",
      "176/176 [==============================] - 0s 215us/sample - loss: 0.4567 - acc: 0.8068\n",
      "Epoch 90/130\n",
      "176/176 [==============================] - 0s 249us/sample - loss: 0.4539 - acc: 0.8125\n",
      "Epoch 91/130\n",
      "176/176 [==============================] - 0s 238us/sample - loss: 0.4496 - acc: 0.8239\n",
      "Epoch 92/130\n",
      "176/176 [==============================] - 0s 391us/sample - loss: 0.4467 - acc: 0.8295\n",
      "Epoch 93/130\n",
      "176/176 [==============================] - 0s 193us/sample - loss: 0.4433 - acc: 0.8295\n",
      "Epoch 94/130\n",
      "176/176 [==============================] - 0s 238us/sample - loss: 0.4402 - acc: 0.8295\n",
      "Epoch 95/130\n",
      "176/176 [==============================] - 0s 232us/sample - loss: 0.4368 - acc: 0.8295\n",
      "Epoch 96/130\n",
      "176/176 [==============================] - 0s 244us/sample - loss: 0.4336 - acc: 0.8295\n",
      "Epoch 97/130\n",
      "176/176 [==============================] - 0s 215us/sample - loss: 0.4310 - acc: 0.8295\n",
      "Epoch 98/130\n",
      "176/176 [==============================] - 0s 227us/sample - loss: 0.4279 - acc: 0.8295\n",
      "Epoch 99/130\n",
      "176/176 [==============================] - 0s 227us/sample - loss: 0.4241 - acc: 0.8295\n",
      "Epoch 100/130\n",
      "176/176 [==============================] - 0s 227us/sample - loss: 0.4216 - acc: 0.8352\n",
      "Epoch 101/130\n",
      "176/176 [==============================] - 0s 227us/sample - loss: 0.4188 - acc: 0.8409\n",
      "Epoch 102/130\n",
      "176/176 [==============================] - 0s 227us/sample - loss: 0.4157 - acc: 0.8409\n",
      "Epoch 103/130\n",
      "176/176 [==============================] - 0s 255us/sample - loss: 0.4127 - acc: 0.8295\n",
      "Epoch 104/130\n",
      "176/176 [==============================] - 0s 289us/sample - loss: 0.4098 - acc: 0.8295\n",
      "Epoch 105/130\n",
      "176/176 [==============================] - 0s 272us/sample - loss: 0.4074 - acc: 0.8295\n",
      "Epoch 106/130\n",
      "176/176 [==============================] - 0s 221us/sample - loss: 0.4044 - acc: 0.8295\n",
      "Epoch 107/130\n",
      "176/176 [==============================] - 0s 232us/sample - loss: 0.4010 - acc: 0.8409\n",
      "Epoch 108/130\n",
      "176/176 [==============================] - 0s 204us/sample - loss: 0.3982 - acc: 0.8409\n",
      "Epoch 109/130\n",
      "176/176 [==============================] - 0s 198us/sample - loss: 0.3948 - acc: 0.8409\n",
      "Epoch 110/130\n",
      "176/176 [==============================] - 0s 249us/sample - loss: 0.3931 - acc: 0.8409\n",
      "Epoch 111/130\n",
      "176/176 [==============================] - 0s 249us/sample - loss: 0.3896 - acc: 0.8352\n",
      "Epoch 112/130\n",
      "176/176 [==============================] - 0s 283us/sample - loss: 0.3869 - acc: 0.8409\n",
      "Epoch 113/130\n",
      "176/176 [==============================] - 0s 329us/sample - loss: 0.3839 - acc: 0.8409\n",
      "Epoch 114/130\n",
      "176/176 [==============================] - 0s 215us/sample - loss: 0.3809 - acc: 0.8409\n",
      "Epoch 115/130\n",
      "176/176 [==============================] - 0s 210us/sample - loss: 0.3790 - acc: 0.8466\n",
      "Epoch 116/130\n",
      "176/176 [==============================] - 0s 238us/sample - loss: 0.3754 - acc: 0.8580\n",
      "Epoch 117/130\n",
      "176/176 [==============================] - 0s 221us/sample - loss: 0.3727 - acc: 0.8636\n",
      "Epoch 118/130\n",
      "176/176 [==============================] - 0s 221us/sample - loss: 0.3702 - acc: 0.8636\n",
      "Epoch 119/130\n",
      "176/176 [==============================] - 0s 255us/sample - loss: 0.3668 - acc: 0.8750\n",
      "Epoch 120/130\n",
      "176/176 [==============================] - 0s 227us/sample - loss: 0.3648 - acc: 0.8636\n",
      "Epoch 121/130\n",
      "176/176 [==============================] - 0s 272us/sample - loss: 0.3620 - acc: 0.8636\n",
      "Epoch 122/130\n",
      "176/176 [==============================] - 0s 227us/sample - loss: 0.3591 - acc: 0.8636\n",
      "Epoch 123/130\n",
      "176/176 [==============================] - 0s 238us/sample - loss: 0.3568 - acc: 0.8693\n",
      "Epoch 124/130\n",
      "176/176 [==============================] - 0s 221us/sample - loss: 0.3537 - acc: 0.8750\n",
      "Epoch 125/130\n",
      "176/176 [==============================] - 0s 221us/sample - loss: 0.3513 - acc: 0.8693\n",
      "Epoch 126/130\n",
      "176/176 [==============================] - 0s 232us/sample - loss: 0.3486 - acc: 0.8750\n",
      "Epoch 127/130\n",
      "176/176 [==============================] - 0s 232us/sample - loss: 0.3459 - acc: 0.8750\n",
      "Epoch 128/130\n",
      "176/176 [==============================] - 0s 278us/sample - loss: 0.3434 - acc: 0.8750\n",
      "Epoch 129/130\n",
      "176/176 [==============================] - 0s 278us/sample - loss: 0.3403 - acc: 0.8807\n",
      "Epoch 130/130\n",
      "176/176 [==============================] - 0s 244us/sample - loss: 0.3385 - acc: 0.8864\n",
      "[[16  5]\n",
      " [ 4 20]]\n",
      "None\n",
      "(0.8, 0.8, 0.8333333333333334, 0.816326530612245, 0.8472222222222223, 0.8911687486738153, 0.8889964728496962)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K.permute_dimensions(WK.shape (64, None)\n",
      "QK.shape (64, 64)\n",
      "(None, 64)\n",
      "K.permute_dimensions(WK.shape (64, None)\n",
      "QK.shape (64, 64)\n",
      "(None, 64)\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " mRNA_inputs (InputLayer)       [(None, 2000)]       0           []                               \n",
      "                                                                                                  \n",
      " miRNA_inputs (InputLayer)      [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " h0_mRNA (Biological_module)    (None, 238)          6087        ['mRNA_inputs[0][0]']            \n",
      "                                                                                                  \n",
      " h0_miRNA (Biological_module)   (None, 238)          10267       ['miRNA_inputs[0][0]']           \n",
      "                                                                                                  \n",
      " self__attention_2 (Self_Attent  (None, 64)          45696       ['h0_mRNA[0][0]']                \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " self__attention_3 (Self_Attent  (None, 64)          45696       ['h0_miRNA[0][0]']               \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 128)          0           ['self__attention_2[0][0]',      \n",
      "                                                                  'self__attention_3[0][0]']      \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 32)           4128        ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1)            33          ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 111,907\n",
      "Trainable params: 111,907\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 177 samples\n",
      "Epoch 1/130\n",
      "177/177 [==============================] - 0s 1ms/sample - loss: 0.9227 - acc: 0.4576\n",
      "Epoch 2/130\n",
      "177/177 [==============================] - 0s 237us/sample - loss: 0.9185 - acc: 0.5819\n",
      "Epoch 3/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.9144 - acc: 0.6045\n",
      "Epoch 4/130\n",
      "177/177 [==============================] - 0s 214us/sample - loss: 0.9104 - acc: 0.6780\n",
      "Epoch 5/130\n",
      "177/177 [==============================] - 0s 242us/sample - loss: 0.9063 - acc: 0.7062\n",
      "Epoch 6/130\n",
      "177/177 [==============================] - 0s 208us/sample - loss: 0.9023 - acc: 0.7288\n",
      "Epoch 7/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.8983 - acc: 0.7458\n",
      "Epoch 8/130\n",
      "177/177 [==============================] - 0s 265us/sample - loss: 0.8944 - acc: 0.7232\n",
      "Epoch 9/130\n",
      "177/177 [==============================] - 0s 214us/sample - loss: 0.8905 - acc: 0.7514\n",
      "Epoch 10/130\n",
      "177/177 [==============================] - 0s 276us/sample - loss: 0.8866 - acc: 0.7458\n",
      "Epoch 11/130\n",
      "177/177 [==============================] - 0s 214us/sample - loss: 0.8827 - acc: 0.7345\n",
      "Epoch 12/130\n",
      "177/177 [==============================] - 0s 242us/sample - loss: 0.8788 - acc: 0.7401\n",
      "Epoch 13/130\n",
      "177/177 [==============================] - 0s 208us/sample - loss: 0.8750 - acc: 0.7401\n",
      "Epoch 14/130\n",
      "177/177 [==============================] - 0s 242us/sample - loss: 0.8711 - acc: 0.7458\n",
      "Epoch 15/130\n",
      "177/177 [==============================] - 0s 276us/sample - loss: 0.8673 - acc: 0.7458\n",
      "Epoch 16/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.8633 - acc: 0.7458\n",
      "Epoch 17/130\n",
      "177/177 [==============================] - 0s 248us/sample - loss: 0.8594 - acc: 0.7345\n",
      "Epoch 18/130\n",
      "177/177 [==============================] - 0s 208us/sample - loss: 0.8554 - acc: 0.7458\n",
      "Epoch 19/130\n",
      "177/177 [==============================] - 0s 254us/sample - loss: 0.8513 - acc: 0.7401\n",
      "Epoch 20/130\n",
      "177/177 [==============================] - 0s 214us/sample - loss: 0.8469 - acc: 0.7458\n",
      "Epoch 21/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.8426 - acc: 0.7458\n",
      "Epoch 22/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.8381 - acc: 0.7401\n",
      "Epoch 23/130\n",
      "177/177 [==============================] - 0s 259us/sample - loss: 0.8332 - acc: 0.7458\n",
      "Epoch 24/130\n",
      "177/177 [==============================] - 0s 276us/sample - loss: 0.8282 - acc: 0.7458\n",
      "Epoch 25/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.8229 - acc: 0.7514\n",
      "Epoch 26/130\n",
      "177/177 [==============================] - 0s 225us/sample - loss: 0.8172 - acc: 0.7458\n",
      "Epoch 27/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.8115 - acc: 0.7345\n",
      "Epoch 28/130\n",
      "177/177 [==============================] - 0s 394us/sample - loss: 0.8049 - acc: 0.7345\n",
      "Epoch 29/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.7984 - acc: 0.7345\n",
      "Epoch 30/130\n",
      "177/177 [==============================] - 0s 237us/sample - loss: 0.7911 - acc: 0.7401\n",
      "Epoch 31/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.7835 - acc: 0.7345\n",
      "Epoch 32/130\n",
      "177/177 [==============================] - 0s 270us/sample - loss: 0.7756 - acc: 0.7401\n",
      "Epoch 33/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.7669 - acc: 0.7288\n",
      "Epoch 34/130\n",
      "177/177 [==============================] - 0s 208us/sample - loss: 0.7580 - acc: 0.7288\n",
      "Epoch 35/130\n",
      "177/177 [==============================] - 0s 265us/sample - loss: 0.7485 - acc: 0.7345\n",
      "Epoch 36/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.7396 - acc: 0.7401\n",
      "Epoch 37/130\n",
      "177/177 [==============================] - 0s 304us/sample - loss: 0.7295 - acc: 0.7458\n",
      "Epoch 38/130\n",
      "177/177 [==============================] - 0s 237us/sample - loss: 0.7197 - acc: 0.7458\n",
      "Epoch 39/130\n",
      "177/177 [==============================] - 0s 248us/sample - loss: 0.7096 - acc: 0.7514\n",
      "Epoch 40/130\n",
      "177/177 [==============================] - 0s 225us/sample - loss: 0.7011 - acc: 0.7627\n",
      "Epoch 41/130\n",
      "177/177 [==============================] - 0s 242us/sample - loss: 0.6915 - acc: 0.7627\n",
      "Epoch 42/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.6818 - acc: 0.7627\n",
      "Epoch 43/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.6732 - acc: 0.7627\n",
      "Epoch 44/130\n",
      "177/177 [==============================] - 0s 225us/sample - loss: 0.6646 - acc: 0.7627\n",
      "Epoch 45/130\n",
      "177/177 [==============================] - 0s 276us/sample - loss: 0.6558 - acc: 0.7514\n",
      "Epoch 46/130\n",
      "177/177 [==============================] - 0s 287us/sample - loss: 0.6482 - acc: 0.7514\n",
      "Epoch 47/130\n",
      "177/177 [==============================] - 0s 254us/sample - loss: 0.6403 - acc: 0.7627\n",
      "Epoch 48/130\n",
      "177/177 [==============================] - 0s 378us/sample - loss: 0.6324 - acc: 0.7627\n",
      "Epoch 49/130\n",
      "177/177 [==============================] - 0s 310us/sample - loss: 0.6255 - acc: 0.7627\n",
      "Epoch 50/130\n",
      "177/177 [==============================] - 0s 197us/sample - loss: 0.6184 - acc: 0.7627\n",
      "Epoch 51/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.6120 - acc: 0.7740\n",
      "Epoch 52/130\n",
      "177/177 [==============================] - 0s 259us/sample - loss: 0.6054 - acc: 0.7740\n",
      "Epoch 53/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.5986 - acc: 0.7853\n",
      "Epoch 54/130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177/177 [==============================] - 0s 231us/sample - loss: 0.5926 - acc: 0.7853\n",
      "Epoch 55/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.5882 - acc: 0.7853\n",
      "Epoch 56/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.5812 - acc: 0.7910\n",
      "Epoch 57/130\n",
      "177/177 [==============================] - 0s 248us/sample - loss: 0.5751 - acc: 0.7853\n",
      "Epoch 58/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.5695 - acc: 0.7853\n",
      "Epoch 59/130\n",
      "177/177 [==============================] - 0s 242us/sample - loss: 0.5642 - acc: 0.7910\n",
      "Epoch 60/130\n",
      "177/177 [==============================] - 0s 225us/sample - loss: 0.5595 - acc: 0.7910\n",
      "Epoch 61/130\n",
      "177/177 [==============================] - 0s 225us/sample - loss: 0.5534 - acc: 0.7910\n",
      "Epoch 62/130\n",
      "177/177 [==============================] - 0s 225us/sample - loss: 0.5481 - acc: 0.7910\n",
      "Epoch 63/130\n",
      "177/177 [==============================] - 0s 242us/sample - loss: 0.5444 - acc: 0.7910\n",
      "Epoch 64/130\n",
      "177/177 [==============================] - 0s 208us/sample - loss: 0.5382 - acc: 0.7966\n",
      "Epoch 65/130\n",
      "177/177 [==============================] - 0s 214us/sample - loss: 0.5331 - acc: 0.8136\n",
      "Epoch 66/130\n",
      "177/177 [==============================] - 0s 242us/sample - loss: 0.5282 - acc: 0.8136\n",
      "Epoch 67/130\n",
      "177/177 [==============================] - 0s 208us/sample - loss: 0.5235 - acc: 0.8136\n",
      "Epoch 68/130\n",
      "177/177 [==============================] - 0s 242us/sample - loss: 0.5189 - acc: 0.8136\n",
      "Epoch 69/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.5135 - acc: 0.8192\n",
      "Epoch 70/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.5095 - acc: 0.8192\n",
      "Epoch 71/130\n",
      "177/177 [==============================] - 0s 254us/sample - loss: 0.5043 - acc: 0.8192\n",
      "Epoch 72/130\n",
      "177/177 [==============================] - 0s 254us/sample - loss: 0.5006 - acc: 0.8192\n",
      "Epoch 73/130\n",
      "177/177 [==============================] - 0s 242us/sample - loss: 0.4957 - acc: 0.8249\n",
      "Epoch 74/130\n",
      "177/177 [==============================] - 0s 214us/sample - loss: 0.4926 - acc: 0.8249\n",
      "Epoch 75/130\n",
      "177/177 [==============================] - 0s 276us/sample - loss: 0.4872 - acc: 0.8305\n",
      "Epoch 76/130\n",
      "177/177 [==============================] - 0s 214us/sample - loss: 0.4825 - acc: 0.8362\n",
      "Epoch 77/130\n",
      "177/177 [==============================] - 0s 259us/sample - loss: 0.4784 - acc: 0.8362\n",
      "Epoch 78/130\n",
      "177/177 [==============================] - 0s 225us/sample - loss: 0.4746 - acc: 0.8305\n",
      "Epoch 79/130\n",
      "177/177 [==============================] - 0s 304us/sample - loss: 0.4701 - acc: 0.8418\n",
      "Epoch 80/130\n",
      "177/177 [==============================] - 0s 214us/sample - loss: 0.4663 - acc: 0.8418\n",
      "Epoch 81/130\n",
      "177/177 [==============================] - 0s 237us/sample - loss: 0.4627 - acc: 0.8418\n",
      "Epoch 82/130\n",
      "177/177 [==============================] - 0s 237us/sample - loss: 0.4584 - acc: 0.8418\n",
      "Epoch 83/130\n",
      "177/177 [==============================] - 0s 242us/sample - loss: 0.4548 - acc: 0.8531\n",
      "Epoch 84/130\n",
      "177/177 [==============================] - 0s 242us/sample - loss: 0.4512 - acc: 0.8588\n",
      "Epoch 85/130\n",
      "177/177 [==============================] - 0s 214us/sample - loss: 0.4474 - acc: 0.8588\n",
      "Epoch 86/130\n",
      "177/177 [==============================] - 0s 242us/sample - loss: 0.4434 - acc: 0.8588\n",
      "Epoch 87/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.4400 - acc: 0.8588\n",
      "Epoch 88/130\n",
      "177/177 [==============================] - 0s 237us/sample - loss: 0.4370 - acc: 0.8531\n",
      "Epoch 89/130\n",
      "177/177 [==============================] - 0s 276us/sample - loss: 0.4331 - acc: 0.8588\n",
      "Epoch 90/130\n",
      "177/177 [==============================] - 0s 282us/sample - loss: 0.4294 - acc: 0.8588\n",
      "Epoch 91/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.4268 - acc: 0.8588\n",
      "Epoch 92/130\n",
      "177/177 [==============================] - 0s 265us/sample - loss: 0.4230 - acc: 0.8644\n",
      "Epoch 93/130\n",
      "177/177 [==============================] - 0s 242us/sample - loss: 0.4199 - acc: 0.8644\n",
      "Epoch 94/130\n",
      "177/177 [==============================] - 0s 248us/sample - loss: 0.4165 - acc: 0.8701\n",
      "Epoch 95/130\n",
      "177/177 [==============================] - 0s 237us/sample - loss: 0.4134 - acc: 0.8588\n",
      "Epoch 96/130\n",
      "177/177 [==============================] - 0s 214us/sample - loss: 0.4103 - acc: 0.8588\n",
      "Epoch 97/130\n",
      "177/177 [==============================] - 0s 282us/sample - loss: 0.4065 - acc: 0.8644\n",
      "Epoch 98/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.4033 - acc: 0.8644\n",
      "Epoch 99/130\n",
      "177/177 [==============================] - 0s 248us/sample - loss: 0.4004 - acc: 0.8701\n",
      "Epoch 100/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.3976 - acc: 0.8701\n",
      "Epoch 101/130\n",
      "177/177 [==============================] - 0s 248us/sample - loss: 0.3943 - acc: 0.8701\n",
      "Epoch 102/130\n",
      "177/177 [==============================] - 0s 214us/sample - loss: 0.3915 - acc: 0.8644\n",
      "Epoch 103/130\n",
      "177/177 [==============================] - 0s 259us/sample - loss: 0.3886 - acc: 0.8701\n",
      "Epoch 104/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.3860 - acc: 0.8757\n",
      "Epoch 105/130\n",
      "177/177 [==============================] - 0s 214us/sample - loss: 0.3819 - acc: 0.8757\n",
      "Epoch 106/130\n",
      "177/177 [==============================] - 0s 259us/sample - loss: 0.3792 - acc: 0.8757\n",
      "Epoch 107/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.3787 - acc: 0.8870\n",
      "Epoch 108/130\n",
      "177/177 [==============================] - 0s 254us/sample - loss: 0.3746 - acc: 0.8870\n",
      "Epoch 109/130\n",
      "177/177 [==============================] - 0s 214us/sample - loss: 0.3706 - acc: 0.8870\n",
      "Epoch 110/130\n",
      "177/177 [==============================] - 0s 248us/sample - loss: 0.3680 - acc: 0.8870\n",
      "Epoch 111/130\n",
      "177/177 [==============================] - 0s 214us/sample - loss: 0.3662 - acc: 0.8927\n",
      "Epoch 112/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.3629 - acc: 0.8927\n",
      "Epoch 113/130\n",
      "177/177 [==============================] - 0s 248us/sample - loss: 0.3599 - acc: 0.8927\n",
      "Epoch 114/130\n",
      "177/177 [==============================] - 0s 366us/sample - loss: 0.3568 - acc: 0.8983\n",
      "Epoch 115/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.3547 - acc: 0.8983\n",
      "Epoch 116/130\n",
      "177/177 [==============================] - 0s 186us/sample - loss: 0.3522 - acc: 0.8983\n",
      "Epoch 117/130\n",
      "177/177 [==============================] - 0s 242us/sample - loss: 0.3490 - acc: 0.9040\n",
      "Epoch 118/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.3477 - acc: 0.8983\n",
      "Epoch 119/130\n",
      "177/177 [==============================] - 0s 248us/sample - loss: 0.3444 - acc: 0.8983\n",
      "Epoch 120/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.3413 - acc: 0.8983\n",
      "Epoch 121/130\n",
      "177/177 [==============================] - 0s 265us/sample - loss: 0.3387 - acc: 0.8983\n",
      "Epoch 122/130\n",
      "177/177 [==============================] - 0s 242us/sample - loss: 0.3358 - acc: 0.9040\n",
      "Epoch 123/130\n",
      "177/177 [==============================] - 0s 225us/sample - loss: 0.3336 - acc: 0.9040\n",
      "Epoch 124/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.3311 - acc: 0.9040\n",
      "Epoch 125/130\n",
      "177/177 [==============================] - 0s 225us/sample - loss: 0.3283 - acc: 0.8983\n",
      "Epoch 126/130\n",
      "177/177 [==============================] - 0s 237us/sample - loss: 0.3256 - acc: 0.9040\n",
      "Epoch 127/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.3233 - acc: 0.9153\n",
      "Epoch 128/130\n",
      "177/177 [==============================] - 0s 259us/sample - loss: 0.3214 - acc: 0.9096\n",
      "Epoch 129/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.3189 - acc: 0.9096\n",
      "Epoch 130/130\n",
      "177/177 [==============================] - 0s 248us/sample - loss: 0.3159 - acc: 0.9153\n",
      "[[13  7]\n",
      " [ 7 17]]\n",
      "None\n",
      "(0.7083333333333334, 0.6818181818181818, 0.7083333333333334, 0.7083333333333334, 0.7833333333333333, 0.8192096013793126, 0.814171470201871)\n",
      "K.permute_dimensions(WK.shape (64, None)\n",
      "QK.shape (64, 64)\n",
      "(None, 64)\n",
      "K.permute_dimensions(WK.shape (64, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QK.shape (64, 64)\n",
      "(None, 64)\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " mRNA_inputs (InputLayer)       [(None, 2000)]       0           []                               \n",
      "                                                                                                  \n",
      " miRNA_inputs (InputLayer)      [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " h0_mRNA (Biological_module)    (None, 238)          6087        ['mRNA_inputs[0][0]']            \n",
      "                                                                                                  \n",
      " h0_miRNA (Biological_module)   (None, 238)          10267       ['miRNA_inputs[0][0]']           \n",
      "                                                                                                  \n",
      " self__attention_4 (Self_Attent  (None, 64)          45696       ['h0_mRNA[0][0]']                \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " self__attention_5 (Self_Attent  (None, 64)          45696       ['h0_miRNA[0][0]']               \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 128)          0           ['self__attention_4[0][0]',      \n",
      "                                                                  'self__attention_5[0][0]']      \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 32)           4128        ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 1)            33          ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 111,907\n",
      "Trainable params: 111,907\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 177 samples\n",
      "Epoch 1/130\n",
      "177/177 [==============================] - 0s 1ms/sample - loss: 0.9230 - acc: 0.4124\n",
      "Epoch 2/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.9188 - acc: 0.5424\n",
      "Epoch 3/130\n",
      "177/177 [==============================] - 0s 276us/sample - loss: 0.9146 - acc: 0.5424\n",
      "Epoch 4/130\n",
      "177/177 [==============================] - 0s 226us/sample - loss: 0.9106 - acc: 0.5424\n",
      "Epoch 5/130\n",
      "177/177 [==============================] - 0s 270us/sample - loss: 0.9065 - acc: 0.5424\n",
      "Epoch 6/130\n",
      "177/177 [==============================] - 0s 242us/sample - loss: 0.9025 - acc: 0.5480\n",
      "Epoch 7/130\n",
      "177/177 [==============================] - 0s 259us/sample - loss: 0.8985 - acc: 0.5537\n",
      "Epoch 8/130\n",
      "177/177 [==============================] - 0s 242us/sample - loss: 0.8946 - acc: 0.5480\n",
      "Epoch 9/130\n",
      "177/177 [==============================] - 0s 254us/sample - loss: 0.8907 - acc: 0.5480\n",
      "Epoch 10/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.8867 - acc: 0.5480\n",
      "Epoch 11/130\n",
      "177/177 [==============================] - 0s 287us/sample - loss: 0.8829 - acc: 0.5593\n",
      "Epoch 12/130\n",
      "177/177 [==============================] - 0s 265us/sample - loss: 0.8790 - acc: 0.5763\n",
      "Epoch 13/130\n",
      "177/177 [==============================] - 0s 259us/sample - loss: 0.8751 - acc: 0.5763\n",
      "Epoch 14/130\n",
      "177/177 [==============================] - 0s 237us/sample - loss: 0.8711 - acc: 0.5819\n",
      "Epoch 15/130\n",
      "177/177 [==============================] - 0s 214us/sample - loss: 0.8672 - acc: 0.5989\n",
      "Epoch 16/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.8633 - acc: 0.6045\n",
      "Epoch 17/130\n",
      "177/177 [==============================] - 0s 197us/sample - loss: 0.8593 - acc: 0.6102\n",
      "Epoch 18/130\n",
      "177/177 [==============================] - 0s 276us/sample - loss: 0.8552 - acc: 0.6328\n",
      "Epoch 19/130\n",
      "177/177 [==============================] - 0s 259us/sample - loss: 0.8511 - acc: 0.6441\n",
      "Epoch 20/130\n",
      "177/177 [==============================] - 0s 270us/sample - loss: 0.8469 - acc: 0.6667\n",
      "Epoch 21/130\n",
      "177/177 [==============================] - 0s 203us/sample - loss: 0.8427 - acc: 0.6497\n",
      "Epoch 22/130\n",
      "177/177 [==============================] - 0s 270us/sample - loss: 0.8381 - acc: 0.6497\n",
      "Epoch 23/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.8336 - acc: 0.6554\n",
      "Epoch 24/130\n",
      "177/177 [==============================] - 0s 208us/sample - loss: 0.8288 - acc: 0.6554\n",
      "Epoch 25/130\n",
      "177/177 [==============================] - 0s 270us/sample - loss: 0.8240 - acc: 0.6723\n",
      "Epoch 26/130\n",
      "177/177 [==============================] - 0s 214us/sample - loss: 0.8186 - acc: 0.6780\n",
      "Epoch 27/130\n",
      "177/177 [==============================] - 0s 237us/sample - loss: 0.8131 - acc: 0.6836\n",
      "Epoch 28/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.8074 - acc: 0.6893\n",
      "Epoch 29/130\n",
      "177/177 [==============================] - 0s 237us/sample - loss: 0.8013 - acc: 0.6949\n",
      "Epoch 30/130\n",
      "177/177 [==============================] - 0s 372us/sample - loss: 0.7949 - acc: 0.7006\n",
      "Epoch 31/130\n",
      "177/177 [==============================] - 0s 270us/sample - loss: 0.7882 - acc: 0.7006\n",
      "Epoch 32/130\n",
      "177/177 [==============================] - 0s 203us/sample - loss: 0.7806 - acc: 0.6949\n",
      "Epoch 33/130\n",
      "177/177 [==============================] - 0s 270us/sample - loss: 0.7729 - acc: 0.7119\n",
      "Epoch 34/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.7652 - acc: 0.7119\n",
      "Epoch 35/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.7571 - acc: 0.7232\n",
      "Epoch 36/130\n",
      "177/177 [==============================] - 0s 276us/sample - loss: 0.7492 - acc: 0.7401\n",
      "Epoch 37/130\n",
      "177/177 [==============================] - 0s 225us/sample - loss: 0.7403 - acc: 0.7401\n",
      "Epoch 38/130\n",
      "177/177 [==============================] - 0s 242us/sample - loss: 0.7327 - acc: 0.7288\n",
      "Epoch 39/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.7234 - acc: 0.7345\n",
      "Epoch 40/130\n",
      "177/177 [==============================] - 0s 270us/sample - loss: 0.7148 - acc: 0.7288\n",
      "Epoch 41/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.7064 - acc: 0.7401\n",
      "Epoch 42/130\n",
      "177/177 [==============================] - 0s 237us/sample - loss: 0.6988 - acc: 0.7288\n",
      "Epoch 43/130\n",
      "177/177 [==============================] - 0s 225us/sample - loss: 0.6902 - acc: 0.7288\n",
      "Epoch 44/130\n",
      "177/177 [==============================] - 0s 299us/sample - loss: 0.6823 - acc: 0.7288\n",
      "Epoch 45/130\n",
      "177/177 [==============================] - 0s 242us/sample - loss: 0.6746 - acc: 0.7288\n",
      "Epoch 46/130\n",
      "177/177 [==============================] - 0s 225us/sample - loss: 0.6672 - acc: 0.7345\n",
      "Epoch 47/130\n",
      "177/177 [==============================] - 0s 208us/sample - loss: 0.6603 - acc: 0.7401\n",
      "Epoch 48/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.6543 - acc: 0.7401\n",
      "Epoch 49/130\n",
      "177/177 [==============================] - 0s 248us/sample - loss: 0.6482 - acc: 0.7345\n",
      "Epoch 50/130\n",
      "177/177 [==============================] - 0s 316us/sample - loss: 0.6419 - acc: 0.7345\n",
      "Epoch 51/130\n",
      "177/177 [==============================] - 0s 214us/sample - loss: 0.6363 - acc: 0.7401\n",
      "Epoch 52/130\n",
      "177/177 [==============================] - 0s 225us/sample - loss: 0.6309 - acc: 0.7514\n",
      "Epoch 53/130\n",
      "177/177 [==============================] - 0s 254us/sample - loss: 0.6257 - acc: 0.7401\n",
      "Epoch 54/130\n",
      "177/177 [==============================] - 0s 254us/sample - loss: 0.6204 - acc: 0.7458\n",
      "Epoch 55/130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177/177 [==============================] - 0s 270us/sample - loss: 0.6168 - acc: 0.7345\n",
      "Epoch 56/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.6109 - acc: 0.7345\n",
      "Epoch 57/130\n",
      "177/177 [==============================] - 0s 265us/sample - loss: 0.6063 - acc: 0.7401\n",
      "Epoch 58/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.6015 - acc: 0.7401\n",
      "Epoch 59/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.5971 - acc: 0.7458\n",
      "Epoch 60/130\n",
      "177/177 [==============================] - 0s 237us/sample - loss: 0.5930 - acc: 0.7571\n",
      "Epoch 61/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.5883 - acc: 0.7514\n",
      "Epoch 62/130\n",
      "177/177 [==============================] - 0s 287us/sample - loss: 0.5838 - acc: 0.7571\n",
      "Epoch 63/130\n",
      "177/177 [==============================] - 0s 203us/sample - loss: 0.5800 - acc: 0.7514\n",
      "Epoch 64/130\n",
      "177/177 [==============================] - 0s 254us/sample - loss: 0.5747 - acc: 0.7514\n",
      "Epoch 65/130\n",
      "177/177 [==============================] - 0s 248us/sample - loss: 0.5708 - acc: 0.7514\n",
      "Epoch 66/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.5661 - acc: 0.7514\n",
      "Epoch 67/130\n",
      "177/177 [==============================] - 0s 254us/sample - loss: 0.5617 - acc: 0.7627\n",
      "Epoch 68/130\n",
      "177/177 [==============================] - 0s 225us/sample - loss: 0.5582 - acc: 0.7684\n",
      "Epoch 69/130\n",
      "177/177 [==============================] - 0s 265us/sample - loss: 0.5538 - acc: 0.7797\n",
      "Epoch 70/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.5490 - acc: 0.7740\n",
      "Epoch 71/130\n",
      "177/177 [==============================] - 0s 248us/sample - loss: 0.5452 - acc: 0.7797\n",
      "Epoch 72/130\n",
      "177/177 [==============================] - 0s 209us/sample - loss: 0.5405 - acc: 0.7797\n",
      "Epoch 73/130\n",
      "177/177 [==============================] - 0s 225us/sample - loss: 0.5363 - acc: 0.7910\n",
      "Epoch 74/130\n",
      "177/177 [==============================] - 0s 242us/sample - loss: 0.5322 - acc: 0.7910\n",
      "Epoch 75/130\n",
      "177/177 [==============================] - 0s 208us/sample - loss: 0.5292 - acc: 0.7910\n",
      "Epoch 76/130\n",
      "177/177 [==============================] - 0s 282us/sample - loss: 0.5238 - acc: 0.7910\n",
      "Epoch 77/130\n",
      "177/177 [==============================] - 0s 282us/sample - loss: 0.5197 - acc: 0.7853\n",
      "Epoch 78/130\n",
      "177/177 [==============================] - 0s 270us/sample - loss: 0.5159 - acc: 0.8023\n",
      "Epoch 79/130\n",
      "177/177 [==============================] - 0s 237us/sample - loss: 0.5123 - acc: 0.7966\n",
      "Epoch 80/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.5089 - acc: 0.8079\n",
      "Epoch 81/130\n",
      "177/177 [==============================] - 0s 203us/sample - loss: 0.5040 - acc: 0.8079\n",
      "Epoch 82/130\n",
      "177/177 [==============================] - 0s 270us/sample - loss: 0.5005 - acc: 0.8079\n",
      "Epoch 83/130\n",
      "177/177 [==============================] - 0s 225us/sample - loss: 0.4960 - acc: 0.8079\n",
      "Epoch 84/130\n",
      "177/177 [==============================] - 0s 276us/sample - loss: 0.4923 - acc: 0.8192\n",
      "Epoch 85/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.4887 - acc: 0.8192\n",
      "Epoch 86/130\n",
      "177/177 [==============================] - 0s 242us/sample - loss: 0.4850 - acc: 0.8136\n",
      "Epoch 87/130\n",
      "177/177 [==============================] - 0s 259us/sample - loss: 0.4817 - acc: 0.8249\n",
      "Epoch 88/130\n",
      "177/177 [==============================] - 0s 225us/sample - loss: 0.4771 - acc: 0.8305\n",
      "Epoch 89/130\n",
      "177/177 [==============================] - 0s 242us/sample - loss: 0.4732 - acc: 0.8305\n",
      "Epoch 90/130\n",
      "177/177 [==============================] - 0s 203us/sample - loss: 0.4700 - acc: 0.8362\n",
      "Epoch 91/130\n",
      "177/177 [==============================] - 0s 361us/sample - loss: 0.4661 - acc: 0.8362\n",
      "Epoch 92/130\n",
      "177/177 [==============================] - 0s 208us/sample - loss: 0.4628 - acc: 0.8362\n",
      "Epoch 93/130\n",
      "177/177 [==============================] - 0s 248us/sample - loss: 0.4592 - acc: 0.8305\n",
      "Epoch 94/130\n",
      "177/177 [==============================] - 0s 209us/sample - loss: 0.4574 - acc: 0.8418\n",
      "Epoch 95/130\n",
      "177/177 [==============================] - 0s 248us/sample - loss: 0.4526 - acc: 0.8418\n",
      "Epoch 96/130\n",
      "177/177 [==============================] - 0s 214us/sample - loss: 0.4489 - acc: 0.8305\n",
      "Epoch 97/130\n",
      "177/177 [==============================] - 0s 254us/sample - loss: 0.4451 - acc: 0.8418\n",
      "Epoch 98/130\n",
      "177/177 [==============================] - 0s 208us/sample - loss: 0.4415 - acc: 0.8418\n",
      "Epoch 99/130\n",
      "177/177 [==============================] - 0s 225us/sample - loss: 0.4389 - acc: 0.8362\n",
      "Epoch 100/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.4351 - acc: 0.8362\n",
      "Epoch 101/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.4315 - acc: 0.8418\n",
      "Epoch 102/130\n",
      "177/177 [==============================] - 0s 265us/sample - loss: 0.4281 - acc: 0.8418\n",
      "Epoch 103/130\n",
      "177/177 [==============================] - 0s 225us/sample - loss: 0.4261 - acc: 0.8418\n",
      "Epoch 104/130\n",
      "177/177 [==============================] - 0s 237us/sample - loss: 0.4220 - acc: 0.8475\n",
      "Epoch 105/130\n",
      "177/177 [==============================] - 0s 282us/sample - loss: 0.4188 - acc: 0.8531\n",
      "Epoch 106/130\n",
      "177/177 [==============================] - 0s 254us/sample - loss: 0.4157 - acc: 0.8531\n",
      "Epoch 107/130\n",
      "177/177 [==============================] - 0s 225us/sample - loss: 0.4123 - acc: 0.8531\n",
      "Epoch 108/130\n",
      "177/177 [==============================] - 0s 259us/sample - loss: 0.4103 - acc: 0.8531\n",
      "Epoch 109/130\n",
      "177/177 [==============================] - 0s 332us/sample - loss: 0.4057 - acc: 0.8588\n",
      "Epoch 110/130\n",
      "177/177 [==============================] - 0s 276us/sample - loss: 0.4030 - acc: 0.8701\n",
      "Epoch 111/130\n",
      "177/177 [==============================] - 0s 214us/sample - loss: 0.4007 - acc: 0.8701\n",
      "Epoch 112/130\n",
      "177/177 [==============================] - 0s 248us/sample - loss: 0.3973 - acc: 0.8757\n",
      "Epoch 113/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.3940 - acc: 0.8701\n",
      "Epoch 114/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.3911 - acc: 0.8701\n",
      "Epoch 115/130\n",
      "177/177 [==============================] - 0s 383us/sample - loss: 0.3884 - acc: 0.8701\n",
      "Epoch 116/130\n",
      "177/177 [==============================] - 0s 248us/sample - loss: 0.3850 - acc: 0.8757\n",
      "Epoch 117/130\n",
      "177/177 [==============================] - 0s 208us/sample - loss: 0.3835 - acc: 0.8814\n",
      "Epoch 118/130\n",
      "177/177 [==============================] - 0s 225us/sample - loss: 0.3795 - acc: 0.8814\n",
      "Epoch 119/130\n",
      "177/177 [==============================] - 0s 293us/sample - loss: 0.3763 - acc: 0.8757\n",
      "Epoch 120/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.3737 - acc: 0.8757\n",
      "Epoch 121/130\n",
      "177/177 [==============================] - 0s 265us/sample - loss: 0.3717 - acc: 0.8814\n",
      "Epoch 122/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.3693 - acc: 0.8814\n",
      "Epoch 123/130\n",
      "177/177 [==============================] - 0s 276us/sample - loss: 0.3650 - acc: 0.8757\n",
      "Epoch 124/130\n",
      "177/177 [==============================] - 0s 237us/sample - loss: 0.3619 - acc: 0.8870\n",
      "Epoch 125/130\n",
      "177/177 [==============================] - 0s 254us/sample - loss: 0.3593 - acc: 0.8814\n",
      "Epoch 126/130\n",
      "177/177 [==============================] - 0s 214us/sample - loss: 0.3575 - acc: 0.8814\n",
      "Epoch 127/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.3541 - acc: 0.8814\n",
      "Epoch 128/130\n",
      "177/177 [==============================] - 0s 225us/sample - loss: 0.3517 - acc: 0.8927\n",
      "Epoch 129/130\n",
      "177/177 [==============================] - 0s 237us/sample - loss: 0.3483 - acc: 0.8927\n",
      "Epoch 130/130\n",
      "177/177 [==============================] - 0s 287us/sample - loss: 0.3463 - acc: 0.8927\n",
      "[[16  4]\n",
      " [ 5 19]]\n",
      "None\n",
      "(0.8260869565217391, 0.7954545454545454, 0.7916666666666666, 0.8085106382978724, 0.8979166666666667, 0.9121256913675208, 0.9097977514684876)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K.permute_dimensions(WK.shape (64, None)\n",
      "QK.shape (64, 64)\n",
      "(None, 64)\n",
      "K.permute_dimensions(WK.shape (64, None)\n",
      "QK.shape (64, 64)\n",
      "(None, 64)\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " mRNA_inputs (InputLayer)       [(None, 2000)]       0           []                               \n",
      "                                                                                                  \n",
      " miRNA_inputs (InputLayer)      [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " h0_mRNA (Biological_module)    (None, 238)          6087        ['mRNA_inputs[0][0]']            \n",
      "                                                                                                  \n",
      " h0_miRNA (Biological_module)   (None, 238)          10267       ['miRNA_inputs[0][0]']           \n",
      "                                                                                                  \n",
      " self__attention_6 (Self_Attent  (None, 64)          45696       ['h0_mRNA[0][0]']                \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " self__attention_7 (Self_Attent  (None, 64)          45696       ['h0_miRNA[0][0]']               \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 128)          0           ['self__attention_6[0][0]',      \n",
      "                                                                  'self__attention_7[0][0]']      \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 32)           4128        ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 1)            33          ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 111,907\n",
      "Trainable params: 111,907\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 177 samples\n",
      "Epoch 1/130\n",
      "177/177 [==============================] - 0s 1ms/sample - loss: 0.9228 - acc: 0.5424\n",
      "Epoch 2/130\n",
      "177/177 [==============================] - 0s 361us/sample - loss: 0.9187 - acc: 0.5424\n",
      "Epoch 3/130\n",
      "177/177 [==============================] - 0s 197us/sample - loss: 0.9146 - acc: 0.5424\n",
      "Epoch 4/130\n",
      "177/177 [==============================] - 0s 259us/sample - loss: 0.9105 - acc: 0.5593\n",
      "Epoch 5/130\n",
      "177/177 [==============================] - 0s 214us/sample - loss: 0.9065 - acc: 0.6102\n",
      "Epoch 6/130\n",
      "177/177 [==============================] - 0s 271us/sample - loss: 0.9025 - acc: 0.6328\n",
      "Epoch 7/130\n",
      "177/177 [==============================] - 0s 203us/sample - loss: 0.8985 - acc: 0.6441\n",
      "Epoch 8/130\n",
      "177/177 [==============================] - 0s 208us/sample - loss: 0.8945 - acc: 0.6610\n",
      "Epoch 9/130\n",
      "177/177 [==============================] - 0s 265us/sample - loss: 0.8906 - acc: 0.6667\n",
      "Epoch 10/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.8867 - acc: 0.6723\n",
      "Epoch 11/130\n",
      "177/177 [==============================] - 0s 265us/sample - loss: 0.8827 - acc: 0.6667\n",
      "Epoch 12/130\n",
      "177/177 [==============================] - 0s 214us/sample - loss: 0.8789 - acc: 0.6667\n",
      "Epoch 13/130\n",
      "177/177 [==============================] - 0s 248us/sample - loss: 0.8750 - acc: 0.6949\n",
      "Epoch 14/130\n",
      "177/177 [==============================] - 0s 214us/sample - loss: 0.8710 - acc: 0.6836\n",
      "Epoch 15/130\n",
      "177/177 [==============================] - 0s 225us/sample - loss: 0.8672 - acc: 0.6836\n",
      "Epoch 16/130\n",
      "177/177 [==============================] - 0s 225us/sample - loss: 0.8633 - acc: 0.6893\n",
      "Epoch 17/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.8593 - acc: 0.6780\n",
      "Epoch 18/130\n",
      "177/177 [==============================] - 0s 259us/sample - loss: 0.8553 - acc: 0.6780\n",
      "Epoch 19/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.8513 - acc: 0.6780\n",
      "Epoch 20/130\n",
      "177/177 [==============================] - 0s 265us/sample - loss: 0.8470 - acc: 0.6893\n",
      "Epoch 21/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.8428 - acc: 0.6893\n",
      "Epoch 22/130\n",
      "177/177 [==============================] - 0s 259us/sample - loss: 0.8386 - acc: 0.6893\n",
      "Epoch 23/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.8341 - acc: 0.6949\n",
      "Epoch 24/130\n",
      "177/177 [==============================] - 0s 208us/sample - loss: 0.8293 - acc: 0.6949\n",
      "Epoch 25/130\n",
      "177/177 [==============================] - 0s 237us/sample - loss: 0.8244 - acc: 0.6893\n",
      "Epoch 26/130\n",
      "177/177 [==============================] - 0s 214us/sample - loss: 0.8192 - acc: 0.7006\n",
      "Epoch 27/130\n",
      "177/177 [==============================] - 0s 265us/sample - loss: 0.8139 - acc: 0.7119\n",
      "Epoch 28/130\n",
      "177/177 [==============================] - 0s 248us/sample - loss: 0.8082 - acc: 0.7175\n",
      "Epoch 29/130\n",
      "177/177 [==============================] - 0s 242us/sample - loss: 0.8021 - acc: 0.7175\n",
      "Epoch 30/130\n",
      "177/177 [==============================] - 0s 355us/sample - loss: 0.7954 - acc: 0.7175\n",
      "Epoch 31/130\n",
      "177/177 [==============================] - 0s 237us/sample - loss: 0.7891 - acc: 0.7288\n",
      "Epoch 32/130\n",
      "177/177 [==============================] - 0s 276us/sample - loss: 0.7821 - acc: 0.7288\n",
      "Epoch 33/130\n",
      "177/177 [==============================] - 0s 237us/sample - loss: 0.7747 - acc: 0.7288\n",
      "Epoch 34/130\n",
      "177/177 [==============================] - 0s 254us/sample - loss: 0.7667 - acc: 0.7232\n",
      "Epoch 35/130\n",
      "177/177 [==============================] - 0s 299us/sample - loss: 0.7590 - acc: 0.7232\n",
      "Epoch 36/130\n",
      "177/177 [==============================] - 0s 248us/sample - loss: 0.7505 - acc: 0.7288\n",
      "Epoch 37/130\n",
      "177/177 [==============================] - 0s 214us/sample - loss: 0.7422 - acc: 0.7288\n",
      "Epoch 38/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.7334 - acc: 0.7288\n",
      "Epoch 39/130\n",
      "177/177 [==============================] - 0s 214us/sample - loss: 0.7246 - acc: 0.7288\n",
      "Epoch 40/130\n",
      "177/177 [==============================] - 0s 248us/sample - loss: 0.7152 - acc: 0.7288\n",
      "Epoch 41/130\n",
      "177/177 [==============================] - 0s 214us/sample - loss: 0.7067 - acc: 0.7288\n",
      "Epoch 42/130\n",
      "177/177 [==============================] - 0s 287us/sample - loss: 0.6981 - acc: 0.7345\n",
      "Epoch 43/130\n",
      "177/177 [==============================] - 0s 254us/sample - loss: 0.6898 - acc: 0.7345\n",
      "Epoch 44/130\n",
      "177/177 [==============================] - 0s 299us/sample - loss: 0.6817 - acc: 0.7345\n",
      "Epoch 45/130\n",
      "177/177 [==============================] - 0s 208us/sample - loss: 0.6737 - acc: 0.7345\n",
      "Epoch 46/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.6660 - acc: 0.7458\n",
      "Epoch 47/130\n",
      "177/177 [==============================] - 0s 225us/sample - loss: 0.6591 - acc: 0.7401\n",
      "Epoch 48/130\n",
      "177/177 [==============================] - 0s 259us/sample - loss: 0.6526 - acc: 0.7401\n",
      "Epoch 49/130\n",
      "177/177 [==============================] - 0s 355us/sample - loss: 0.6447 - acc: 0.7458\n",
      "Epoch 50/130\n",
      "177/177 [==============================] - 0s 203us/sample - loss: 0.6383 - acc: 0.7458\n",
      "Epoch 51/130\n",
      "177/177 [==============================] - 0s 225us/sample - loss: 0.6331 - acc: 0.7458\n",
      "Epoch 52/130\n",
      "177/177 [==============================] - 0s 225us/sample - loss: 0.6263 - acc: 0.7458\n",
      "Epoch 53/130\n",
      "177/177 [==============================] - 0s 254us/sample - loss: 0.6194 - acc: 0.7627\n",
      "Epoch 54/130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177/177 [==============================] - 0s 237us/sample - loss: 0.6136 - acc: 0.7627\n",
      "Epoch 55/130\n",
      "177/177 [==============================] - 0s 270us/sample - loss: 0.6081 - acc: 0.7627\n",
      "Epoch 56/130\n",
      "177/177 [==============================] - 0s 203us/sample - loss: 0.6031 - acc: 0.7571\n",
      "Epoch 57/130\n",
      "177/177 [==============================] - 0s 242us/sample - loss: 0.5961 - acc: 0.7684\n",
      "Epoch 58/130\n",
      "177/177 [==============================] - 0s 225us/sample - loss: 0.5907 - acc: 0.7684\n",
      "Epoch 59/130\n",
      "177/177 [==============================] - 0s 203us/sample - loss: 0.5852 - acc: 0.7684\n",
      "Epoch 60/130\n",
      "177/177 [==============================] - 0s 237us/sample - loss: 0.5798 - acc: 0.7684\n",
      "Epoch 61/130\n",
      "177/177 [==============================] - 0s 192us/sample - loss: 0.5739 - acc: 0.7684\n",
      "Epoch 62/130\n",
      "177/177 [==============================] - 0s 248us/sample - loss: 0.5693 - acc: 0.7571\n",
      "Epoch 63/130\n",
      "177/177 [==============================] - 0s 265us/sample - loss: 0.5641 - acc: 0.7797\n",
      "Epoch 64/130\n",
      "177/177 [==============================] - 0s 270us/sample - loss: 0.5575 - acc: 0.7797\n",
      "Epoch 65/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.5535 - acc: 0.7797\n",
      "Epoch 66/130\n",
      "177/177 [==============================] - 0s 276us/sample - loss: 0.5484 - acc: 0.7910\n",
      "Epoch 67/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.5443 - acc: 0.7853\n",
      "Epoch 68/130\n",
      "177/177 [==============================] - 0s 248us/sample - loss: 0.5389 - acc: 0.7910\n",
      "Epoch 69/130\n",
      "177/177 [==============================] - 0s 208us/sample - loss: 0.5358 - acc: 0.7966\n",
      "Epoch 70/130\n",
      "177/177 [==============================] - 0s 214us/sample - loss: 0.5292 - acc: 0.7966\n",
      "Epoch 71/130\n",
      "177/177 [==============================] - 0s 282us/sample - loss: 0.5233 - acc: 0.8023\n",
      "Epoch 72/130\n",
      "177/177 [==============================] - 0s 225us/sample - loss: 0.5187 - acc: 0.8079\n",
      "Epoch 73/130\n",
      "177/177 [==============================] - 0s 276us/sample - loss: 0.5151 - acc: 0.8079\n",
      "Epoch 74/130\n",
      "177/177 [==============================] - 0s 208us/sample - loss: 0.5109 - acc: 0.8079\n",
      "Epoch 75/130\n",
      "177/177 [==============================] - 0s 310us/sample - loss: 0.5060 - acc: 0.8079\n",
      "Epoch 76/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.5014 - acc: 0.8079\n",
      "Epoch 77/130\n",
      "177/177 [==============================] - 0s 276us/sample - loss: 0.4974 - acc: 0.8192\n",
      "Epoch 78/130\n",
      "177/177 [==============================] - 0s 208us/sample - loss: 0.4925 - acc: 0.8192\n",
      "Epoch 79/130\n",
      "177/177 [==============================] - 0s 259us/sample - loss: 0.4883 - acc: 0.8249\n",
      "Epoch 80/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.4846 - acc: 0.8192\n",
      "Epoch 81/130\n",
      "177/177 [==============================] - 0s 242us/sample - loss: 0.4803 - acc: 0.8192\n",
      "Epoch 82/130\n",
      "177/177 [==============================] - 0s 208us/sample - loss: 0.4761 - acc: 0.8192\n",
      "Epoch 83/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.4729 - acc: 0.8249\n",
      "Epoch 84/130\n",
      "177/177 [==============================] - 0s 248us/sample - loss: 0.4686 - acc: 0.8362\n",
      "Epoch 85/130\n",
      "177/177 [==============================] - 0s 214us/sample - loss: 0.4645 - acc: 0.8305\n",
      "Epoch 86/130\n",
      "177/177 [==============================] - 0s 254us/sample - loss: 0.4609 - acc: 0.8305\n",
      "Epoch 87/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.4569 - acc: 0.8418\n",
      "Epoch 88/130\n",
      "177/177 [==============================] - 0s 225us/sample - loss: 0.4535 - acc: 0.8475\n",
      "Epoch 89/130\n",
      "177/177 [==============================] - 0s 237us/sample - loss: 0.4501 - acc: 0.8362\n",
      "Epoch 90/130\n",
      "177/177 [==============================] - 0s 361us/sample - loss: 0.4465 - acc: 0.8362\n",
      "Epoch 91/130\n",
      "177/177 [==============================] - 0s 208us/sample - loss: 0.4426 - acc: 0.8418\n",
      "Epoch 92/130\n",
      "177/177 [==============================] - 0s 254us/sample - loss: 0.4384 - acc: 0.8418\n",
      "Epoch 93/130\n",
      "177/177 [==============================] - 0s 237us/sample - loss: 0.4347 - acc: 0.8588\n",
      "Epoch 94/130\n",
      "177/177 [==============================] - 0s 203us/sample - loss: 0.4321 - acc: 0.8644\n",
      "Epoch 95/130\n",
      "177/177 [==============================] - 0s 242us/sample - loss: 0.4296 - acc: 0.8757\n",
      "Epoch 96/130\n",
      "177/177 [==============================] - 0s 226us/sample - loss: 0.4256 - acc: 0.8757\n",
      "Epoch 97/130\n",
      "177/177 [==============================] - 0s 259us/sample - loss: 0.4225 - acc: 0.8588\n",
      "Epoch 98/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.4185 - acc: 0.8588\n",
      "Epoch 99/130\n",
      "177/177 [==============================] - 0s 237us/sample - loss: 0.4167 - acc: 0.8757\n",
      "Epoch 100/130\n",
      "177/177 [==============================] - 0s 214us/sample - loss: 0.4117 - acc: 0.8870\n",
      "Epoch 101/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.4086 - acc: 0.8870\n",
      "Epoch 102/130\n",
      "177/177 [==============================] - 0s 237us/sample - loss: 0.4056 - acc: 0.8701\n",
      "Epoch 103/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.4021 - acc: 0.8814\n",
      "Epoch 104/130\n",
      "177/177 [==============================] - 0s 265us/sample - loss: 0.3987 - acc: 0.8870\n",
      "Epoch 105/130\n",
      "177/177 [==============================] - 0s 265us/sample - loss: 0.3954 - acc: 0.8927\n",
      "Epoch 106/130\n",
      "177/177 [==============================] - 0s 225us/sample - loss: 0.3925 - acc: 0.8927\n",
      "Epoch 107/130\n",
      "177/177 [==============================] - 0s 208us/sample - loss: 0.3897 - acc: 0.8927\n",
      "Epoch 108/130\n",
      "177/177 [==============================] - 0s 203us/sample - loss: 0.3864 - acc: 0.8927\n",
      "Epoch 109/130\n",
      "177/177 [==============================] - 0s 276us/sample - loss: 0.3841 - acc: 0.8814\n",
      "Epoch 110/130\n",
      "177/177 [==============================] - 0s 225us/sample - loss: 0.3803 - acc: 0.8814\n",
      "Epoch 111/130\n",
      "177/177 [==============================] - 0s 254us/sample - loss: 0.3812 - acc: 0.8814\n",
      "Epoch 112/130\n",
      "177/177 [==============================] - 0s 214us/sample - loss: 0.3740 - acc: 0.8983\n",
      "Epoch 113/130\n",
      "177/177 [==============================] - 0s 242us/sample - loss: 0.3722 - acc: 0.8814\n",
      "Epoch 114/130\n",
      "177/177 [==============================] - 0s 338us/sample - loss: 0.3690 - acc: 0.8814\n",
      "Epoch 115/130\n",
      "177/177 [==============================] - 0s 225us/sample - loss: 0.3664 - acc: 0.8814\n",
      "Epoch 116/130\n",
      "177/177 [==============================] - 0s 225us/sample - loss: 0.3635 - acc: 0.8927\n",
      "Epoch 117/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.3606 - acc: 0.8927\n",
      "Epoch 118/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.3572 - acc: 0.8927\n",
      "Epoch 119/130\n",
      "177/177 [==============================] - 0s 248us/sample - loss: 0.3547 - acc: 0.8983\n",
      "Epoch 120/130\n",
      "177/177 [==============================] - 0s 203us/sample - loss: 0.3515 - acc: 0.9040\n",
      "Epoch 121/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.3498 - acc: 0.8870\n",
      "Epoch 122/130\n",
      "177/177 [==============================] - 0s 242us/sample - loss: 0.3465 - acc: 0.8870\n",
      "Epoch 123/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.3437 - acc: 0.8983\n",
      "Epoch 124/130\n",
      "177/177 [==============================] - 0s 242us/sample - loss: 0.3409 - acc: 0.8983\n",
      "Epoch 125/130\n",
      "177/177 [==============================] - 0s 208us/sample - loss: 0.3380 - acc: 0.8983\n",
      "Epoch 126/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.3363 - acc: 0.9096\n",
      "Epoch 127/130\n",
      "177/177 [==============================] - 0s 242us/sample - loss: 0.3330 - acc: 0.9040\n",
      "Epoch 128/130\n",
      "177/177 [==============================] - 0s 225us/sample - loss: 0.3303 - acc: 0.9096\n",
      "Epoch 129/130\n",
      "177/177 [==============================] - 0s 265us/sample - loss: 0.3297 - acc: 0.9153\n",
      "Epoch 130/130\n",
      "177/177 [==============================] - 0s 254us/sample - loss: 0.3258 - acc: 0.9096\n",
      "[[16  4]\n",
      " [ 4 20]]\n",
      "None\n",
      "(0.8333333333333334, 0.8181818181818182, 0.8333333333333334, 0.8333333333333334, 0.9, 0.9273570163711418, 0.9259084508259251)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K.permute_dimensions(WK.shape (64, None)\n",
      "QK.shape (64, 64)\n",
      "(None, 64)\n",
      "K.permute_dimensions(WK.shape (64, None)\n",
      "QK.shape (64, 64)\n",
      "(None, 64)\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " mRNA_inputs (InputLayer)       [(None, 2000)]       0           []                               \n",
      "                                                                                                  \n",
      " miRNA_inputs (InputLayer)      [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " h0_mRNA (Biological_module)    (None, 238)          6087        ['mRNA_inputs[0][0]']            \n",
      "                                                                                                  \n",
      " h0_miRNA (Biological_module)   (None, 238)          10267       ['miRNA_inputs[0][0]']           \n",
      "                                                                                                  \n",
      " self__attention_8 (Self_Attent  (None, 64)          45696       ['h0_mRNA[0][0]']                \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " self__attention_9 (Self_Attent  (None, 64)          45696       ['h0_miRNA[0][0]']               \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 128)          0           ['self__attention_8[0][0]',      \n",
      "                                                                  'self__attention_9[0][0]']      \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 32)           4128        ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 1)            33          ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 111,907\n",
      "Trainable params: 111,907\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 177 samples\n",
      "Epoch 1/130\n",
      "177/177 [==============================] - 0s 1ms/sample - loss: 0.9227 - acc: 0.4859\n",
      "Epoch 2/130\n",
      "177/177 [==============================] - 0s 361us/sample - loss: 0.9184 - acc: 0.5424\n",
      "Epoch 3/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.9143 - acc: 0.5424\n",
      "Epoch 4/130\n",
      "177/177 [==============================] - 0s 248us/sample - loss: 0.9101 - acc: 0.5706\n",
      "Epoch 5/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.9060 - acc: 0.6667\n",
      "Epoch 6/130\n",
      "177/177 [==============================] - 0s 248us/sample - loss: 0.9019 - acc: 0.7232\n",
      "Epoch 7/130\n",
      "177/177 [==============================] - 0s 214us/sample - loss: 0.8979 - acc: 0.7175\n",
      "Epoch 8/130\n",
      "177/177 [==============================] - 0s 299us/sample - loss: 0.8938 - acc: 0.7232\n",
      "Epoch 9/130\n",
      "177/177 [==============================] - 0s 208us/sample - loss: 0.8898 - acc: 0.7232\n",
      "Epoch 10/130\n",
      "177/177 [==============================] - 0s 248us/sample - loss: 0.8857 - acc: 0.7401\n",
      "Epoch 11/130\n",
      "177/177 [==============================] - 0s 242us/sample - loss: 0.8816 - acc: 0.7288\n",
      "Epoch 12/130\n",
      "177/177 [==============================] - 0s 265us/sample - loss: 0.8775 - acc: 0.7175\n",
      "Epoch 13/130\n",
      "177/177 [==============================] - 0s 214us/sample - loss: 0.8733 - acc: 0.7175\n",
      "Epoch 14/130\n",
      "177/177 [==============================] - 0s 248us/sample - loss: 0.8692 - acc: 0.7232\n",
      "Epoch 15/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.8648 - acc: 0.7062\n",
      "Epoch 16/130\n",
      "177/177 [==============================] - 0s 259us/sample - loss: 0.8605 - acc: 0.7119\n",
      "Epoch 17/130\n",
      "177/177 [==============================] - 0s 242us/sample - loss: 0.8559 - acc: 0.7175\n",
      "Epoch 18/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.8512 - acc: 0.7175\n",
      "Epoch 19/130\n",
      "177/177 [==============================] - 0s 259us/sample - loss: 0.8466 - acc: 0.7119\n",
      "Epoch 20/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.8413 - acc: 0.7175\n",
      "Epoch 21/130\n",
      "177/177 [==============================] - 0s 265us/sample - loss: 0.8360 - acc: 0.7232\n",
      "Epoch 22/130\n",
      "177/177 [==============================] - 0s 214us/sample - loss: 0.8301 - acc: 0.7345\n",
      "Epoch 23/130\n",
      "177/177 [==============================] - 0s 237us/sample - loss: 0.8242 - acc: 0.7345\n",
      "Epoch 24/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.8177 - acc: 0.7232\n",
      "Epoch 25/130\n",
      "177/177 [==============================] - 0s 209us/sample - loss: 0.8109 - acc: 0.7345\n",
      "Epoch 26/130\n",
      "177/177 [==============================] - 0s 242us/sample - loss: 0.8036 - acc: 0.7401\n",
      "Epoch 27/130\n",
      "177/177 [==============================] - 0s 225us/sample - loss: 0.7956 - acc: 0.7571\n",
      "Epoch 28/130\n",
      "177/177 [==============================] - 0s 242us/sample - loss: 0.7877 - acc: 0.7571\n",
      "Epoch 29/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.7782 - acc: 0.7571\n",
      "Epoch 30/130\n",
      "177/177 [==============================] - 0s 327us/sample - loss: 0.7690 - acc: 0.7401\n",
      "Epoch 31/130\n",
      "177/177 [==============================] - 0s 237us/sample - loss: 0.7593 - acc: 0.7458\n",
      "Epoch 32/130\n",
      "177/177 [==============================] - 0s 259us/sample - loss: 0.7491 - acc: 0.7345\n",
      "Epoch 33/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.7391 - acc: 0.7345\n",
      "Epoch 34/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.7284 - acc: 0.7345\n",
      "Epoch 35/130\n",
      "177/177 [==============================] - 0s 214us/sample - loss: 0.7186 - acc: 0.7345\n",
      "Epoch 36/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.7084 - acc: 0.7345\n",
      "Epoch 37/130\n",
      "177/177 [==============================] - 0s 299us/sample - loss: 0.6977 - acc: 0.7345\n",
      "Epoch 38/130\n",
      "177/177 [==============================] - 0s 225us/sample - loss: 0.6880 - acc: 0.7345\n",
      "Epoch 39/130\n",
      "177/177 [==============================] - 0s 270us/sample - loss: 0.6794 - acc: 0.7345\n",
      "Epoch 40/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.6700 - acc: 0.7401\n",
      "Epoch 41/130\n",
      "177/177 [==============================] - 0s 259us/sample - loss: 0.6609 - acc: 0.7458\n",
      "Epoch 42/130\n",
      "177/177 [==============================] - 0s 248us/sample - loss: 0.6532 - acc: 0.7458\n",
      "Epoch 43/130\n",
      "177/177 [==============================] - 0s 237us/sample - loss: 0.6453 - acc: 0.7514\n",
      "Epoch 44/130\n",
      "177/177 [==============================] - 0s 225us/sample - loss: 0.6378 - acc: 0.7458\n",
      "Epoch 45/130\n",
      "177/177 [==============================] - 0s 248us/sample - loss: 0.6306 - acc: 0.7458\n",
      "Epoch 46/130\n",
      "177/177 [==============================] - 0s 270us/sample - loss: 0.6235 - acc: 0.7514\n",
      "Epoch 47/130\n",
      "177/177 [==============================] - 0s 259us/sample - loss: 0.6175 - acc: 0.7571\n",
      "Epoch 48/130\n",
      "177/177 [==============================] - 0s 237us/sample - loss: 0.6107 - acc: 0.7627\n",
      "Epoch 49/130\n",
      "177/177 [==============================] - 0s 349us/sample - loss: 0.6048 - acc: 0.7627\n",
      "Epoch 50/130\n",
      "177/177 [==============================] - 0s 192us/sample - loss: 0.5993 - acc: 0.7684\n",
      "Epoch 51/130\n",
      "177/177 [==============================] - 0s 248us/sample - loss: 0.5928 - acc: 0.7740\n",
      "Epoch 52/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.5881 - acc: 0.7797\n",
      "Epoch 53/130\n",
      "177/177 [==============================] - 0s 265us/sample - loss: 0.5817 - acc: 0.7853\n",
      "Epoch 54/130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177/177 [==============================] - 0s 270us/sample - loss: 0.5771 - acc: 0.7853\n",
      "Epoch 55/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.5715 - acc: 0.7853\n",
      "Epoch 56/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.5660 - acc: 0.7797\n",
      "Epoch 57/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.5610 - acc: 0.7966\n",
      "Epoch 58/130\n",
      "177/177 [==============================] - 0s 242us/sample - loss: 0.5558 - acc: 0.8023\n",
      "Epoch 59/130\n",
      "177/177 [==============================] - 0s 254us/sample - loss: 0.5515 - acc: 0.8023\n",
      "Epoch 60/130\n",
      "177/177 [==============================] - 0s 282us/sample - loss: 0.5462 - acc: 0.8079\n",
      "Epoch 61/130\n",
      "177/177 [==============================] - 0s 259us/sample - loss: 0.5409 - acc: 0.8079\n",
      "Epoch 62/130\n",
      "177/177 [==============================] - 0s 265us/sample - loss: 0.5365 - acc: 0.8136\n",
      "Epoch 63/130\n",
      "177/177 [==============================] - 0s 248us/sample - loss: 0.5329 - acc: 0.8136\n",
      "Epoch 64/130\n",
      "177/177 [==============================] - 0s 276us/sample - loss: 0.5269 - acc: 0.8079\n",
      "Epoch 65/130\n",
      "177/177 [==============================] - 0s 208us/sample - loss: 0.5222 - acc: 0.8079\n",
      "Epoch 66/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.5181 - acc: 0.8079\n",
      "Epoch 67/130\n",
      "177/177 [==============================] - 0s 242us/sample - loss: 0.5131 - acc: 0.8079\n",
      "Epoch 68/130\n",
      "177/177 [==============================] - 0s 208us/sample - loss: 0.5087 - acc: 0.8136\n",
      "Epoch 69/130\n",
      "177/177 [==============================] - 0s 242us/sample - loss: 0.5040 - acc: 0.8192\n",
      "Epoch 70/130\n",
      "177/177 [==============================] - 0s 254us/sample - loss: 0.5020 - acc: 0.8136\n",
      "Epoch 71/130\n",
      "177/177 [==============================] - 0s 242us/sample - loss: 0.4955 - acc: 0.8192\n",
      "Epoch 72/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.4918 - acc: 0.8249\n",
      "Epoch 73/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.4870 - acc: 0.8305\n",
      "Epoch 74/130\n",
      "177/177 [==============================] - 0s 259us/sample - loss: 0.4828 - acc: 0.8362\n",
      "Epoch 75/130\n",
      "177/177 [==============================] - 0s 208us/sample - loss: 0.4788 - acc: 0.8418\n",
      "Epoch 76/130\n",
      "177/177 [==============================] - 0s 259us/sample - loss: 0.4744 - acc: 0.8418\n",
      "Epoch 77/130\n",
      "177/177 [==============================] - 0s 237us/sample - loss: 0.4705 - acc: 0.8362\n",
      "Epoch 78/130\n",
      "177/177 [==============================] - 0s 265us/sample - loss: 0.4661 - acc: 0.8418\n",
      "Epoch 79/130\n",
      "177/177 [==============================] - 0s 225us/sample - loss: 0.4620 - acc: 0.8418\n",
      "Epoch 80/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.4581 - acc: 0.8418\n",
      "Epoch 81/130\n",
      "177/177 [==============================] - 0s 265us/sample - loss: 0.4548 - acc: 0.8475\n",
      "Epoch 82/130\n",
      "177/177 [==============================] - 0s 259us/sample - loss: 0.4501 - acc: 0.8475\n",
      "Epoch 83/130\n",
      "177/177 [==============================] - 0s 237us/sample - loss: 0.4464 - acc: 0.8475\n",
      "Epoch 84/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.4424 - acc: 0.8475\n",
      "Epoch 85/130\n",
      "177/177 [==============================] - 0s 248us/sample - loss: 0.4387 - acc: 0.8475\n",
      "Epoch 86/130\n",
      "177/177 [==============================] - 0s 225us/sample - loss: 0.4348 - acc: 0.8475\n",
      "Epoch 87/130\n",
      "177/177 [==============================] - 0s 265us/sample - loss: 0.4311 - acc: 0.8531\n",
      "Epoch 88/130\n",
      "177/177 [==============================] - 0s 423us/sample - loss: 0.4273 - acc: 0.8531\n",
      "Epoch 89/130\n",
      "177/177 [==============================] - 0s 242us/sample - loss: 0.4240 - acc: 0.8531\n",
      "Epoch 90/130\n",
      "177/177 [==============================] - 0s 214us/sample - loss: 0.4199 - acc: 0.8588\n",
      "Epoch 91/130\n",
      "177/177 [==============================] - 0s 225us/sample - loss: 0.4166 - acc: 0.8588\n",
      "Epoch 92/130\n",
      "177/177 [==============================] - 0s 214us/sample - loss: 0.4133 - acc: 0.8588\n",
      "Epoch 93/130\n",
      "177/177 [==============================] - 0s 259us/sample - loss: 0.4096 - acc: 0.8644\n",
      "Epoch 94/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.4050 - acc: 0.8701\n",
      "Epoch 95/130\n",
      "177/177 [==============================] - 0s 259us/sample - loss: 0.4023 - acc: 0.8644\n",
      "Epoch 96/130\n",
      "177/177 [==============================] - 0s 265us/sample - loss: 0.3987 - acc: 0.8701\n",
      "Epoch 97/130\n",
      "177/177 [==============================] - 0s 225us/sample - loss: 0.3946 - acc: 0.8757\n",
      "Epoch 98/130\n",
      "177/177 [==============================] - 0s 265us/sample - loss: 0.3937 - acc: 0.8701\n",
      "Epoch 99/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.3888 - acc: 0.8757\n",
      "Epoch 100/130\n",
      "177/177 [==============================] - 0s 276us/sample - loss: 0.3849 - acc: 0.8814\n",
      "Epoch 101/130\n",
      "177/177 [==============================] - 0s 248us/sample - loss: 0.3814 - acc: 0.8927\n",
      "Epoch 102/130\n",
      "177/177 [==============================] - 0s 293us/sample - loss: 0.3779 - acc: 0.8927\n",
      "Epoch 103/130\n",
      "177/177 [==============================] - 0s 231us/sample - loss: 0.3744 - acc: 0.8927\n",
      "Epoch 104/130\n",
      "177/177 [==============================] - 0s 276us/sample - loss: 0.3711 - acc: 0.8927\n",
      "Epoch 105/130\n",
      "177/177 [==============================] - 0s 214us/sample - loss: 0.3688 - acc: 0.8983\n",
      "Epoch 106/130\n",
      "177/177 [==============================] - 0s 254us/sample - loss: 0.3640 - acc: 0.9040\n",
      "Epoch 107/130\n",
      "177/177 [==============================] - 0s 225us/sample - loss: 0.3607 - acc: 0.9040\n",
      "Epoch 108/130\n",
      "177/177 [==============================] - 0s 254us/sample - loss: 0.3574 - acc: 0.9096\n",
      "Epoch 109/130\n",
      "177/177 [==============================] - 0s 225us/sample - loss: 0.3547 - acc: 0.9096\n",
      "Epoch 110/130\n",
      "177/177 [==============================] - 0s 270us/sample - loss: 0.3513 - acc: 0.9096\n",
      "Epoch 111/130\n",
      "177/177 [==============================] - 0s 248us/sample - loss: 0.3478 - acc: 0.9096\n",
      "Epoch 112/130\n",
      "177/177 [==============================] - 0s 349us/sample - loss: 0.3442 - acc: 0.9153\n",
      "Epoch 113/130\n",
      "177/177 [==============================] - 0s 208us/sample - loss: 0.3423 - acc: 0.9209\n",
      "Epoch 114/130\n",
      "177/177 [==============================] - 0s 237us/sample - loss: 0.3379 - acc: 0.9209\n",
      "Epoch 115/130\n",
      "177/177 [==============================] - 0s 209us/sample - loss: 0.3350 - acc: 0.9209\n",
      "Epoch 116/130\n",
      "177/177 [==============================] - 0s 220us/sample - loss: 0.3322 - acc: 0.9209\n",
      "Epoch 117/130\n",
      "177/177 [==============================] - 0s 270us/sample - loss: 0.3287 - acc: 0.9209\n",
      "Epoch 118/130\n",
      "177/177 [==============================] - 0s 214us/sample - loss: 0.3259 - acc: 0.9209\n",
      "Epoch 119/130\n",
      "177/177 [==============================] - 0s 265us/sample - loss: 0.3231 - acc: 0.9153\n",
      "Epoch 120/130\n",
      "177/177 [==============================] - 0s 203us/sample - loss: 0.3192 - acc: 0.9266\n",
      "Epoch 121/130\n",
      "177/177 [==============================] - 0s 254us/sample - loss: 0.3164 - acc: 0.9209\n",
      "Epoch 122/130\n",
      "177/177 [==============================] - 0s 208us/sample - loss: 0.3136 - acc: 0.9266\n",
      "Epoch 123/130\n",
      "177/177 [==============================] - 0s 214us/sample - loss: 0.3103 - acc: 0.9266\n",
      "Epoch 124/130\n",
      "177/177 [==============================] - 0s 259us/sample - loss: 0.3073 - acc: 0.9322\n",
      "Epoch 125/130\n",
      "177/177 [==============================] - 0s 237us/sample - loss: 0.3045 - acc: 0.9322\n",
      "Epoch 126/130\n",
      "177/177 [==============================] - 0s 276us/sample - loss: 0.3012 - acc: 0.9322\n",
      "Epoch 127/130\n",
      "177/177 [==============================] - 0s 237us/sample - loss: 0.2981 - acc: 0.9379\n",
      "Epoch 128/130\n",
      "177/177 [==============================] - 0s 225us/sample - loss: 0.2958 - acc: 0.9379\n",
      "Epoch 129/130\n",
      "177/177 [==============================] - 0s 225us/sample - loss: 0.2950 - acc: 0.9379\n",
      "Epoch 130/130\n",
      "177/177 [==============================] - 0s 276us/sample - loss: 0.2896 - acc: 0.9435\n",
      "[[13  7]\n",
      " [ 5 19]]\n",
      "None\n",
      "(0.7307692307692307, 0.7272727272727273, 0.7916666666666666, 0.76, 0.8104166666666667, 0.8610414306636518, 0.8580836433113261)\n",
      "Cross validated results :  ACC = 0.7645454545454546, REC = 0.7916666666666667, F1 = 0.7853007671153568, AUC = 0.8477777777777777, AUPR =0.8821804976910885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    }
   ],
   "source": [
    "#Five-fold cross validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=1030) \n",
    "\n",
    "kfscore = []\n",
    "p = 0\n",
    "for train_index, test_index in skf.split(mRNA_data.values,y):\n",
    "\n",
    "\n",
    "    mRNA_train_x = mRNA_data.values[train_index]\n",
    "    mRNA_test_x  = mRNA_data.values[test_index]\n",
    "\n",
    "    miRNA_train_x = miRNA_data.values[train_index]\n",
    "    miRNA_test_x  = miRNA_data.values[test_index]\n",
    "\n",
    "    train_y  = y[train_index]\n",
    "    test_y   = y[test_index]\n",
    "\n",
    "    model = create_model(mRNA_data,miRNA_data)\n",
    "    model.fit( { \"mRNA_inputs\": mRNA_train_x, 'miRNA_inputs':miRNA_train_x},train_y,\n",
    "                 epochs=130,batch_size = 64,class_weight = {0:x_0,1:x_1})  \n",
    "\n",
    "    y_pred = model.predict({\"mRNA_inputs\": mRNA_test_x,'miRNA_inputs':miRNA_test_x})\n",
    "\n",
    "    y_score = [1 if index>=0.5  else 0 for index in  y_pred]\n",
    "\n",
    "    evaluate_epoch = get_metrics(test_y,y_score,y_pred)\n",
    "    print(evaluate_epoch)\n",
    "\n",
    "    kfscore.append(evaluate_epoch)\n",
    "    \n",
    "results = list(np.array(kfscore).sum(axis= 0)/5.0)\n",
    "print('Cross validated results :  ACC = {}, REC = {}, F1 = {}, AUC = {}, AUPR ={}'.format(results[1],results[2],results[3],results[4],results[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b58a98f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e535d96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pnet_env",
   "language": "python",
   "name": "pnet_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
